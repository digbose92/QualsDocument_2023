\chapter{Conclusion and ongoing work}
\label{cha:conclusion}

Here, we would like to revisit the thesis statement before setting directions for ongoing/future work.

\begin{tcolorbox}[width=\textwidth]
Context-guided attention improves multi-modal content understanding at diverse scales.
\end{tcolorbox}


Till now, we have developed models based on context-guided attention for macro/instance level content understanding tasks w.r.t. domains of movies, TV shows, natural scenes, and advertisements. The content understanding tasks are reliant on the representations learned through context-guided attention mechanisms. Here, we would like to pose the following question regarding the enhancement of these learned representations:

\begin{itshape}
Q: Can we guide the representations learned through context-guided attention to be optimal yet robust for the given macro/instance level task?
\label{context:macro instance level}
\end{itshape}

In terms of guidance, we would like to consider the available knowledge from pretrained multimodal models. With the increase in web-based data sources, there is a trend toward the curation of large-scale weakly aligned datasets consisting of images/videos and noisy descriptions like LAION-5B \cite{schuhmann2022laionb}, CC-12M \cite{changpinyo2021cc12m}, WebVid \cite{Bain21}. The curation of these large-scale sources has led to the rise of multimodal pretraining \cite{wang2022MMPTMSurvey}, where transformer-based models are guided by certain objectives as follows:
\begin{itemize}

  \item \textbf{Image-text matching:} Detect whether the image and text pairs are aligned with each other  
 \item \textbf{Word-region alignment:} Detect whether certain words in the descriptions are matched to image regions
 \item \textbf{Caption generation:} Generate a natural language caption to match the given description with the image
 
\end{itemize}

The aforementioned pretraining tasks are certain examples of objectives used for large-scale models and by no means comprise an exhaustive list. The pretraining stage is followed by subsequent finetuning operations for a wide variety of downstream classification and generative tasks, as mentioned below:

\begin{itemize}

    \item \textbf{Classification tasks:} Visual-question answering, Video-language inference, Visual entailment, Category recognition, Multi-modal sentiment analysis
    \item \textbf{Generative tasks:} Image/Video captioning, Visual dialogue, Multimodal machine translation
    
\end{itemize}
 An outline of the pretraining and finetuning operation is shown in Fig \ref{multimodal pretraining}.

 \begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/multimodal_pretraining.png}
    \caption{Pretraining and finetuning operation pipeline in multimodal models.}
    \label{multimodal pretraining}
 \end{figure}

Hence we would like to recast the previous mentioned question to include the role of multimodal pretrained knowledge as follows:

\begin{itshape}
Q: Can we leverage existing large-scale multimodal knowledge to obtain optimal yet robust context-guided representations for given macro/instance-level tasks?
\end{itshape}

With this question, we introduce the concept of Information Bottleneck and how it can be used to obtain optimal yet robust representations for a given task.

\section{Information Bottleneck}

The information bottleneck principle \cite{Tishby2015DeepLA} formulates the goal of deep learning as a trade-off between compression and predictive power. If a given task has associated inputs $X$ and labels $Y$ and the intermediate layer representations of a deep learning model are denoted by $T$, then the optimal task-specific representation can be obtained through the following lagrangian formulation:

\begin{equation}
    L_{IB}= I(Y;T) - \beta I(X;T)
\end{equation}

Here, the objective aims at 
\begin{itemize}
    \item \textbf{Increasing predicting power:} Maximize the mutual information (MI) between labels $Y$ and representations $T$. 
    \item \textbf{Representation compression:} Minimize the mutual information (MI) between inputs $X$ and representations $T$.
\end{itemize}
The parameter $\beta$ controls the relative contributions of the two objective terms. In our case, we would like to explore the usage of information bottleneck as a regularization objective for pretrained multimodal models and its impact on macro and instance-level content understanding tasks under a variety of input settings. We outline our proposed work in the following sections:

\subsection{Related work}
Information Bottleneck (IB) has been utilized for low-resource finetuning of BERT for natural language inference and sentiment analysis tasks in \cite{Mahabadi2021VariationalIB}. Additional usage of information bottleneck includes its usage as task-specific regularizer for learning adversarially robust representations in language models \cite{wang2021infobert}. In the domain of multimodal learning, IB as a regularizer has enabled learning of multimodal representations robust to linguistic variations, image corruptions for visual question answering task \cite{Jiang2022CorrelationIB}. 

\subsection{Information bottleneck and multimodality}

\begin{figure}
     \centering
    \includegraphics[width=\textwidth]{figures/multimodal_IB.png}
    \caption{Multimodal architecture outline for information bottleneck.}
    \label{multimodal pretraining}
\end{figure}

The base multimodal pretrained architecture to be considered for information bottleneck-related study is shown in Fig \ref{multimodal pretraining} . Given two modalities $m_{1}$ and $m_{2}$, the sequence of operations can be listed as follows:

\begin{align}
    T_{m_{1}}&=f_{m_{1}}(X_{m_{1}}), T_{m_{2}}=f_{m_{2}}(X_{m_{2}})\\
    Y&=F_{m_{1}m_{2}}([T_{m_{1}},T_{m_{2}}])
\end{align}
Here $T_{m_{i}}$ refers to the input embeddings for modality $m_{i}$ and $F_{m_{1}m_{2}}$ refers to the multimodal encoder for predicting the labels. For the multimodal setting, the information bottleneck regularizer can be written as follows:

\begin{equation}
    L_{IB}= I(Y;\{T_{m_{1}},T_{m_{2}}\}) - \beta I(\{X_{m_{1}},X_{m_{2}}\};\{T_{m_{1}},T_{m_{2}}\})
\end{equation}

\subsection{Proposed work (A) - Optimal task-specific representations}
\label{Optimal task-specific representations}
In the first part of the proposed work, we would like to benchmark the performance of various multimodal models under the impact of information bottleneck for content understanding tasks. Since finetuning multimodal models is a memory-intensive operation, we would like to explore the parameter settings under which optimal task-specific representations can be extracted after using the information bottleneck as a regularizer. The outline of the proposed work can be found in Fig \ref{optimal_task_representations}.
The context-processing operation extracts contextual information from the given multimodal content through various modalities. The processed contextual information is passed as input to a pretrained multimodal model with a wide variety of parameter settings:
\begin {itemize}
\item \textbf{Linear-Probe:} Finetuning of the task-specific classifier head only and complete freezing of the multimodal encoder.
\item \textbf{Parameter efficient techniques:} Usage of parameter efficient techniques like LoRA, Adapters, prefix tuning for the multimodal encoder
\item \textbf{Full finetuning:} Complete finetuning of the multimodal encoder.
\end{itemize}
For the above parameter settings, we would like to investigate the impact of information bottleneck guidance while finetuning for a host of macro and instance-level tasks as follows:

\begin{itemize}
    \item \textbf{Advertisement video understanding:} Proposed MM-AU Benchmark
    \item \textbf{Hateful content detection:} Hateful memes \cite{Kiela2020TheHM} dataset
    \item \textbf{Movie understanding:}  Movie genre classification \cite{2019Moviescope}, MMIMDB \cite{Arevalo2017GatedMU}
    \item \textbf {Human affect understanding:} Emotic \cite{kostiPAMI}, CAER \cite{CAER-S}
\end{itemize}
\begin{figure}
 \centering 
 \includegraphics[width=\textwidth]{figures/optimal_task_representations.png}
 \caption{Optimal macro and instance-level task representations based on information bottleneck under different parameter settings}
 \label{optimal_task_representations}
\end{figure}

In terms of architecture, we plan to use the following based on different modality combinations:
\begin{itemize}
\item \textbf{Vision-language:} BLIP-2 \cite{Li2023BLIP2BL}, ViLT \cite{Kim2021ViLTVT}
\item \textbf{Audio-visual:} UAVM model \cite{uavm_gong}
\end{itemize}


\subsection{Proposed work (B) - Characterizing robustness}

Our previous approaches regarding macro and instance level content understanding relied on the assumption that the input sources are free from corruption. However, in real-life settings, the modalities associated with input sources undergo corruption as mentioned in the following examples:

\begin{itemize}
 \item \textbf{Acquisition failure:}  Failure of camera sensors in traffic intersections.
 \item \textbf{Privacy concerns:} Privacy restrictions on accessing social media posts or chats.
 \item \textbf{Noise corruptions:} Excessive noise in audio recordings to be used for emotion classification
\end{itemize}

In the second part of our proposed work, we plan to use the information bottleneck regularizer under the settings of corrupted modalities for pretrained multimodal models. The overall goal is to determine the robustness of the pretrained models with and without information bottleneck regularizer during finetuning. An outline of the corrupted modality setting with our previous optimal task-specific representation framework based on information bottleneck (IB) is shown in Fig \ref{robustness_multimodal_models}.

\begin{figure}
 \centering 
 \includegraphics[width=\textwidth]{figures/robustness_multimodal_models.png}
 \caption{Optimal macro and instance-level task representations based on information bottleneck under different parameter settings}
 \label{robustness_multimodal_models}
\end{figure}

We plan to consider the following training and testing scenarios in the robustness study setting:

\begin{itemize}

\item \textbf{Scenario A:} Finetune multimodal model under different parameter settings with IB regularizer and non-corrupted modality inputs.
\item \textbf{Scenario B:} Finetune multimodal model under different parameter settings with IB regularizer and corrupted modality inputs
\item \textbf{Scenario C:} Finetune multimodal model under different parameter settings with IB regularizer and mutual information (MI) maximization between non-corrupted inputs and multimodal representations.

\end{itemize}

In the above-mentioned scenarios, the evaluation will be performed using corrupted modality data. We plan to use the datasets and architectures mentioned in the previous section for this study as well. 
