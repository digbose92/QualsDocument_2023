
In today's information-rich landscape, there has been a rapid rise in content available through web-based platforms reliant on diverse modalities. Multimodal content can manifest in varied forms like online news articles, movies, advertisements, and digital short videos. Our ability to comprehend the underlying narratives in these sources depends on the integration of various modalities, including audio, visual, and language. Further proliferation of multimodal content across diverse platforms necessitates a large-scale understanding of its impact on individuals and society as a whole. 
\par
In this work, we consider the problem of multimodal content understanding at diverse scales to obtain a holistic view of the input sources. Further, our approach to content understanding is reliant on contextual information processing from multimodal input sources. We process diverse forms of contextual information through various modalities and integrate the same using context-guided attention mechanisms. We show that the inclusion of context information improves multi-scale content understanding across a variety of input sources like movies, advertisements, TV shows, and natural scene images. We further conclude by providing directions for future research where we would like to explore information theoretic approaches for extracting optimal representations from context-guided attention mechanisms.






