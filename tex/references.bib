%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for ZHANG Ruda at 2016-04-04 14:14:11 -0700


%% Saved with string encoding Unicode (UTF-8)


@article{einstein_ist_1905,
	title = {Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?},
	volume = {323},
	rights = {Copyright © 1905 {WILEY}‐{VCH} Verlag {GmbH} \& Co. {KGaA}, Weinheim},
	issn = {1521-3889},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/andp.19053231314},
	doi = {10.1002/andp.19053231314},
	pages = {639--641},
	number = {13},
	journaltitle = {Annalen der Physik},
	author = {Einstein, A.},
	date = {1905}
}

@book{Fisher:1954,
	Address = {New York},
	Author = {Ronald Aylmer Fisher},
	Edition = {12th},
	Publisher = {Hafner},
	Title = {Statistical methods for research workers},
	Year = {1954}}

@article{Robbins:1951aa,
	Author = {Robbins, Herbert and Sutton Monro},
	Journal = {The Annals of Mathematical Statistics},
	Number = {3},
	Pages = {pp. 400-407},
	Title = {A Stochastic Approximation Method},
	Volume = {22},
	Year = {1951}}

@book{Knight:1921,
	Address = {Boston, New York},
	Author = {Frank H Knight},
	Publisher = {Houghton Mifflin Company},
	Title = {Risk, uncertainty and profit},
	Year = {1921}}

@article{Wright:1921aa,
	Author = {Wright, Sewall},
	Journal = {Journal of agricultural research},
	Pages = {557-585},
	Title = {Correlation and Causation},
	Year = {1921}}

@article{Caratheodory:1909aa,
	Author = {Constantin Carath{\'e}odory},
	Journal = {Mathematische Annalen},
	Pages = {355--386},
	Title = {Untersuchungen {\"u}ber die Grundlagen der Thermodynamik},
	Volume = {67},
	Year = {1909}}

@book{Gibbs:1902,
	Address = {New York},
	Author = {Gibbs, J Willard},
	Publisher = {C. Scribner's sons},
	Title = {Elementary principles in statistical mechanics: Developed with especial reference to the rational foundations of thermodynamics},
	Year = {1902}}

@article{Clausius:1857,
	Author = {Clausius, Rudolf},
	Journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	Number = {91},
	Pages = {108--127},
	Title = {XI. On the nature of the motion which we call heat},
	Volume = {14},
	Year = {1857}}

@article{contextvision,
author = {Wang, Xuan and Zhu, Zhigang},
title = {Context Understanding in Computer Vision: A Survey},
year = {2023},
issue_date = {Mar 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {229},
number = {C},
issn = {1077-3142},
url = {https://doi-org.libproxy2.usc.edu/10.1016/j.cviu.2023.103646},
doi = {10.1016/j.cviu.2023.103646},
journal = {Comput. Vis. Image Underst.},
month = {apr},
numpages = {28},
keywords = {65D17, Deep Learning Models, Context, 41A10, 41A05, Computer Vision, Context Integration, 65D05}
}

@article{Rabinovich2007ObjectsIC,
  title={Objects in Context},
  author={Andrew Rabinovich and Andrea Vedaldi and Carolina Galleguillos and Eric Wiewiora and Serge J. Belongie},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:749550}
}

@article{Wang2007ShapeAA,
  title={Shape and Appearance Context Modeling},
  author={Xiaogang Wang and Gianfranco Doretto and Thomas B. Sebastian and J. Rittscher and Peter H. Tu},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:14958192}
}

@ARTICLE {Faceness-Net,
author = {S. Yang and P. Luo and C. Loy and X. Tang},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Faceness-Net: Face Detection through Deep Facial Part Responses},
year = {2018},
volume = {40},
number = {08},
issn = {1939-3539},
pages = {1845-1859},
abstract = {We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.},
keywords = {face;proposals;face detection;detectors;neural networks;mouth;training},
doi = {10.1109/TPAMI.2017.2738644},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@article{Marques2010ContextMI,
  title={Context modeling in computer vision: techniques, implications, and applications},
  author={Oge Marques and Elan Barenholtz and Vincent Charvillat},
  journal={Multimedia Tools and Applications},
  year={2010},
  volume={51},
  pages={303-339},
  url={https://api.semanticscholar.org/CorpusID:18206222}
}

@article{Chang2021ACS,
  title={A Comprehensive Survey of Scene Graphs: Generation and Application},
  author={Xiaojun Chang and Pengzhen Ren and Pengfei Xu and Zhihui Li and Xiaojiang Chen and Alexander G. Hauptmann},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={45},
  pages={1-26},
  url={https://api.semanticscholar.org/CorpusID:245445853}
}

@article{Xu2017SceneGG,
  title={Scene Graph Generation by Iterative Message Passing},
  author={Danfei Xu and Yuke Zhu and Christopher Bongsoo Choy and Li Fei-Fei},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={3097-3106},
  url={https://api.semanticscholar.org/CorpusID:1780254}
}

@article{Yan2019LearningCG,
  title={Learning Context Graph for Person Search},
  author={Yichao Yan and Qiang Zhang and Bingbing Ni and Wendong Zhang and Minghao Xu and Xiaokang Yang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2153-2162},
  url={https://api.semanticscholar.org/CorpusID:102487416}
}

@article{Carreira2017QuoVA,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733},
  url={https://api.semanticscholar.org/CorpusID:206596127}
}

@article{Ji2021DetectingHR,
  title={Detecting Human-Object Relationships in Videos},
  author={Jingwei Ji and Rishi Desai and Juan Carlos Niebles},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={8086-8096},
  url={https://api.semanticscholar.org/CorpusID:242213105}
}

@article{Wu2021TowardsLV,
  title={Towards Long-Form Video Understanding},
  author={Chaoxia Wu and Philipp Kr{\"a}henb{\"u}hl},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={1884-1894},
  url={https://api.semanticscholar.org/CorpusID:235416690}
}

@article{Soldan2021MADAS,
  title={MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},
  author={Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={5016-5025},
  url={https://api.semanticscholar.org/CorpusID:244773187}
}

@article{Beery2019ContextRL,
  title={Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection},
  author={Sara Beery and Guanhang Wu and Vivek Rathod and Ronny Votel and Jonathan Huang},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={13072-13082},
  url={https://api.semanticscholar.org/CorpusID:208921095}
}

@article{purushwalkam2020audio,
  title={Audio-visual floorplan reconstruction},
  author={Purushwalkam, Senthil and Gari, Sebastian Vicenc Amengual and Ithapu, Vamsi Krishna and Schissler, Carl and Robinson, Philip and Gupta, Abhinav and Grauman, Kristen},
  journal={arXiv preprint arXiv:2012.15470},
  year={2020}
}

@inproceedings{chen2020soundspaces,
    title = {SoundSpaces: Audio-Visual Navigation in 3D Environments},
    author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
    year = {2020},
    booktitle={ECCV},
}

@article{Tian2018AudioVisualEL,
  title={Audio-Visual Event Localization in Unconstrained Videos},
  author={Yapeng Tian and Jing Shi and Bochen Li and Zhiyao Duan and Chenliang Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08842},
  url={https://api.semanticscholar.org/CorpusID:4336836}
}

@article{Zhang2022TemporalSG,
  title={Temporal Sentence Grounding in Videos: A Survey and Future Directions},
  author={Hao Zhang and Aixin Sun and Wei Jing and Joey Tianyi Zhou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  volume={45},
  pages={10443-10465},
  url={https://api.semanticscholar.org/CorpusID:253224445}
}

@article{Seymour2017AutomatedDA,
  title={Automated detection and enumeration of marine wildlife using unmanned aircraft systems (UAS) and thermal imagery},
  author={Alexander C. Seymour and Julian Dale and Mike O. Hammill and Patrick N. Halpin and David W. Johnston},
  journal={Scientific Reports},
  year={2017},
  volume={7},
  url={https://api.semanticscholar.org/CorpusID:14140341}
}

@article{Baltruaitis2017MultimodalML,
  title={Multimodal Machine Learning: A Survey and Taxonomy},
  author={Tadas Baltru{\vs}aitis and Chaitanya Ahuja and Louis-Philippe Morency},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  volume={41},
  pages={423-443},
  url={https://api.semanticscholar.org/CorpusID:10137425}
}

@article{Liang2022FoundationsAR,
  title={Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  author={Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.03430},
  url={https://api.semanticscholar.org/CorpusID:252118396}
}

@article{Gao2020ASO,
  title={A Survey on Deep Learning for Multimodal Data Fusion},
  author={Jing Gao and Peng Li and Zhikui Chen and Jianing Zhang},
  journal={Neural Computation},
  year={2020},
  volume={32},
  pages={829-864},
  url={https://api.semanticscholar.org/CorpusID:212748233}
}

@article{Abdar2023ARO,
  title={A Review of Deep Learning for Video Captioning},
  author={Moloud Abdar and Meenakshi Kollati and Swaraja Kuraparthi and Farhad Pourpanah and Daniel J. McDuff and Mohammad Ghavamzadeh and Shuicheng Yan and Abduallah A. Mohamed and Abbas Khosravi and E. Cambria and Fatih Murat Porikli},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11431},
  url={https://api.semanticscholar.org/CorpusID:258298547}
}

@article{Apostolidis2021VideoSU,
  title={Video Summarization Using Deep Neural Networks: A Survey},
  author={Evlampios Apostolidis and E. Adamantidou and Alexandros I. Metsai and Vasileios Mezaris and I. Patras},
  journal={Proceedings of the IEEE},
  year={2021},
  volume={109},
  pages={1838-1863},
  url={https://api.semanticscholar.org/CorpusID:231627658}
}

@misc{defaria2023visual,
      title={Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature}, 
      author={Ana Cláudia Akemi Matsuki de Faria and Felype de Castro Bastos and José Victor Nogueira Alves da Silva and Vitor Lopes Fabris and Valeska de Sousa Uchoa and Décio Gonçalves de Aguiar Neto and Claudio Filipi Goncalves dos Santos},
      year={2023},
      eprint={2305.11033},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Alain2001WhatA,
  title={“What” and “where” in the human auditory system},
  author={Claude Alain and Stephen R. Arnott and Stephanie J. Hevenor and Stephen Graham and Cheryl L. Grady},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2001},
  volume={98},
  pages={12301 - 12306},
  url={https://api.semanticscholar.org/CorpusID:25136641}
}

@article{BornkesselSchlesewsky2015NeurobiologicalRO,
  title={Neurobiological roots of language in primate audition: common computational properties},
  author={Ina Bornkessel-Schlesewsky and Matthias Schlesewsky and Steven L. Small and Josef P. Rauschecker},
  journal={Trends in Cognitive Sciences},
  year={2015},
  volume={19},
  pages={142-150},
  url={https://api.semanticscholar.org/CorpusID:11071265}
}

@article{Wallace2002HistochemicalIO,
  title={Histochemical identification of cortical areas in the auditory region of the human brain},
  author={Mark N. Wallace and Peter W. Johnston and Alan R. Palmer},
  journal={Experimental Brain Research},
  year={2002},
  volume={143},
  pages={499-508},
  url={https://api.semanticscholar.org/CorpusID:24211906}
}

@article{Torralba2003ContextualPF,
  title={Contextual Priming for Object Detection},
  author={Antonio Torralba},
  journal={International Journal of Computer Vision},
  year={2003},
  volume={53},
  pages={169-191},
  url={https://api.semanticscholar.org/CorpusID:1073705}
}

@article{Choi2012ContextMA,
  title={Context models and out-of-context objects},
  author={Myung Jin Choi and Antonio Torralba and Alan S. Willsky},
  journal={Pattern Recognit. Lett.},
  year={2012},
  volume={33},
  pages={853-862},
  url={https://api.semanticscholar.org/CorpusID:8354810}
}

@article{barrettcontext,
author = {Lisa Feldman Barrett and Batja Mesquita and Maria Gendron},
title ={Context in Emotion Perception},
journal = {Current Directions in Psychological Science},
volume = {20},
number = {5},
pages = {286-290},
year = {2011},
doi = {10.1177/0963721411422522},

URL = { 
        https://doi.org/10.1177/0963721411422522
    
},
eprint = { 
        https://doi.org/10.1177/0963721411422522
    
}
,
    abstract = { We review recent work demonstrating consistent context effects during emotion perception. Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotion is perceived in a face, calling into question the still-common assumption that the emotional state of a person is written on and can be read from the face like words on a page. Incorporating context during emotion perception appears to be routine, efficient, and, to some degree, automatic. This evidence challenges the standard view of emotion perception represented in psychology texts, in the cognitive neuroscience literature, and in the popular media and points to a necessary change in the basic paradigm used in the scientific study of emotion perception. }
}

@inproceedings{Zhao2022M3EDMM,
  title={M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database},
  author={Jinming Zhao and Tenggan Zhang and Jingwen Hu and Yuchen Liu and Qin Jin and Xinchao Wang and Haizhou Li},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248780564}
}

@article{Das2016VisualD,
  title={Visual Dialog},
  author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos{\'e} M. F. Moura and Devi Parikh and Dhruv Batra},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1080-1089},
  url={https://api.semanticscholar.org/CorpusID:1820614}
}

@ARTICLE{CMI,
  author={Somandepalli, Krishna and Guha, Tanaya and Martinez, Victor R. and Kumar, Naveen and Adam, Hartwig and Narayanan, Shrikanth},
  journal={Proceedings of the IEEE}, 
  title={Computational Media Intelligence: Human-Centered Machine Analysis of Media}, 
  year={2021},
  volume={109},
  number={5},
  pages={891-910},
  doi={10.1109/JPROC.2020.3047978}}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231591445}
}

@article{Bar2004VisualOI,
  title={Visual objects in context},
  author={Moshe Bar},
  journal={Nature Reviews Neuroscience},
  year={2004},
  volume={5},
  pages={617-629},
  url={https://api.semanticscholar.org/CorpusID:205499985}
}

@article{Qiao2021ObjectLevelSC,
  title={Object-Level Scene Context Prediction},
  author={Xiaotian Qiao and Quanlong Zheng and Ying Cao and Rynson W. H. Lau},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={44},
  pages={5280-5292},
  url={https://api.semanticscholar.org/CorpusID:233428370}
}


@ARTICLE{CMI,
  author={Somandepalli, Krishna and Guha, Tanaya and Martinez, Victor R. and Kumar, Naveen and Adam, Hartwig and Narayanan, Shrikanth},
  journal={Proceedings of the IEEE}, 
  title={Computational Media Intelligence: Human-Centered Machine Analysis of Media}, 
  year={2021},
  volume={109},
  number={5},
  pages={891-910},
  doi={10.1109/JPROC.2020.3047978}}

@inproceedings{Bordwell1979FilmAA,
  title={Film Art: An Introduction},
  author={David Bordwell and Kristin Thompson},
  year={1979},
  url={https://api.semanticscholar.org/CorpusID:190106329}
}

@article{Xiao2010SUNDL,
  title={SUN database: Large-scale scene recognition from abbey to zoo},
  author={Jianxiong Xiao and James Hays and Krista A. Ehinger and Aude Oliva and Antonio Torralba},
  journal={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year={2010},
  pages={3485-3492},
  url={https://api.semanticscholar.org/CorpusID:1309931}
}

@article{zhou2017places,
  title={Places: A 10 million Image Database for Scene Recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@misc{bain2020condensed,
    title={Condensed Movies: Story Based Retrieval with Contextual Embeddings},
    author={Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman},
    year={2020},
    eprint={2005.04208},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{Sadhu_2021_CVPR,
          author = {Sadhu, Arka and Gupta, Tanmay and Yatskar, Mark and Nevatia, Ram and Kembhavi, Aniruddha},
          title = {Visual Semantic Role Labeling for Video Understanding},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
          month = {June},
          year = {2021}}


@InProceedings{SBD,
author="Helm, Daniel
and Kampel, Martin",
editor="Cristani, Marco
and Prati, Andrea
and Lanz, Oswald
and Messelodi, Stefano
and Sebe, Nicu",
title="Shot Boundary Detection for Automatic Video Analysis of Historical Films",
booktitle="New Trends in Image Analysis and Processing -- ICIAP 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="137--147",
abstract="In automatic video content analysis and film preservation, Shot Boundary Detection (SBD) is a fundamental pre-processing step. While previous research focuses on detecting Abrupt Transitions (AT) as well as Gradual Transitions (GT) in different video genres such as sports movies or news clips only few studies investigate in the detection of shot transitions in historical footage. The main aim of this paper is to create an SBD mechanism inspired by state-of-the-art algorithms which is applied and evaluated on a self-generated historical dataset as well as a publicly available dataset called Clipshots. Therefore, a three-stage pipeline is introduced consisting of a Candidate Frame Range Selection based on the network DeepSBD, Extraction of Convolutional Neural Network (CNN) Features and Similarity Calculation. A combination of pre-trained backbone CNNs such as ResNet, VGG19 and SqueezeNet with different similarity metrics like Cosine Similarity and Euclidean Distance are used and evaluated. The outcome of this paper displays that the proposed algorithm reaches promising results on detecting ATs in historical videos without the need of complex optimization and re-training processes. Furthermore, it points out the main challenges concerning historical footage such as damaged film reels, scratches or splices. The results of this paper contribute a significant base for future research on automatic video analysis of historical videos.",
isbn="978-3-030-30754-7"
}

@article{CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               others},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {CoRR},
  volume    = {abs/2103.00020},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  eprinttype = {arXiv},
  eprint    = {2103.00020},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@article{2019Moviescope,
title={Moviescope: Large-scale Analysis of Movies using Multiple Modalities},
author={Paola Cascante-Bonilla and Kalpathy Sitaraman and Mengjia Luo and Vicente Ordonez},
journal={ArXiv},
year={2019},
volume={abs/1908.03180}
}

@INPROCEEDINGS{IndoorScenes,
  author={Quattoni, Ariadna and Torralba, Antonio},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Recognizing indoor scenes}, 
  year={2009},
  volume={},
  number={},
  pages={413-420},
  doi={10.1109/CVPR.2009.5206537}}

@article{xiao_sun_2016,
	title = {{SUN} {Database}: {Exploring} a {Large} {Collection} of {Scene} {Categories}},
	volume = {119},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0748-y},
	doi = {10.1007/s11263-014-0748-y},
	abstract = {Progress in scene understanding requires reasoning about the rich and diverse visual environments that make up our daily experience. To this end, we propose the Scene Understanding database, a nearly exhaustive collection of scenes categorized at the same level of specificity as human discourse. The database contains 908 distinct scene categories and 131,072 images. Given this data with both scene and object labels available, we perform in-depth analysis of co-occurrence statistics and the contextual relationship. To better understand this large scale taxonomy of scene categories, we perform two human experiments: we quantify human scene recognition accuracy, and we measure how typical each image is of its assigned scene category. Next, we perform computational experiments: scene recognition with global image features, indoor versus outdoor classification, and “scene detection,” in which we relax the assumption that one image depicts only one scene category. Finally, we relate human experiments to machine performance and explore the relationship between human and machine recognition errors and the relationship between image “typicality” and machine recognition accuracy.},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Xiao, Jianxiong and Ehinger, Krista A. and Hays, James and Torralba, Antonio and Oliva, Aude},
	month = aug,
	year = {2016},
	pages = {3--22},
}

@article{kinetics400,
  author    = {Will Kay and
               Jo{\~{a}}o Carreira and
               Karen Simonyan and
               Brian Zhang and
               Chloe Hillier and
               Sudheendra Vijayanarasimhan and
               Fabio Viola and
               Tim Green and
               Trevor Back and
               Paul Natsev and
               Mustafa Suleyman and
               Andrew Zisserman},
  title     = {The Kinetics Human Action Video Dataset},
  journal   = {CoRR},
  volume    = {abs/1705.06950},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06950},
  eprinttype = {arXiv},
  eprint    = {1705.06950},
  timestamp = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KayCSZHVVGBNSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@article{Kinetics700,
  author    = {Jo{\~{a}}o Carreira and
               Eric Noland and
               Chloe Hillier and
               Andrew Zisserman},
  title     = {A Short Note on the Kinetics-700 Human Action Dataset},
  journal   = {CoRR},
  volume    = {abs/1907.06987},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.06987},
  eprinttype = {arXiv},
  eprint    = {1907.06987},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-06987.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6047--6056},
  year={2018}
}
  
@article{Something-SomethingV2,
  author    = {Raghav Goyal and
               Samira Ebrahimi Kahou and
               Vincent Michalski and
               Joanna Materzynska and
               Susanne Westphal and others},
  title     = {The "something something" video database for learning and evaluating
               visual common sense},
  journal   = {CoRR},
  volume    = {abs/1706.04261},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04261},
  eprinttype = {arXiv},
  eprint    = {1706.04261},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalKMMWKHFYMH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{SOA,
author="Ray, Jamie
and Wang, Heng
and Tran, Du
and Wang, Yufei
and Feiszli, Matt
and Torresani, Lorenzo
and Paluri, Manohar",
title="Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="660--676",
abstract="This paper introduces a large-scale, multi-label and multi-task video dataset named Scenes-Objects-Actions (SOA). Most prior video datasets are based on a predefined taxonomy, which is used to define the keyword queries issued to search engines. The videos retrieved by the search engines are then verified for correctness by human annotators. Datasets collected in this manner tend to generate high classification accuracy as search engines typically rank ``easy'' videos first. The SOA dataset adopts a different approach. We rely on uniform sampling to get a better representation of videos on the Web. Trained annotators are asked to provide free-form text labels describing each video in three different aspects: scene, object and action. These raw labels are then merged, split and renamed to generate a taxonomy for SOA. All the annotations are verified again based on the taxonomy. The final dataset includes 562K videos with 3.64M annotations spanning 49 categories for scenes, 356 for objects, 148 for actions, and naturally captures the long tail distribution of visual concepts in the real world. We show that datasets collected in this way are quite challenging by evaluating existing popular video models on SOA. We provide in-depth analysis about the performance of different models on SOA, and highlight potential new directions in video classification. We compare SOA with existing datasets and discuss various factors that impact the performance of transfer learning. A key-feature of SOA is that it enables the empirical study of correlation among scene, object and action recognition in video. We present results of this study and further analyze the potential of using the information learned from one task to improve the others. We also demonstrate different ways of scaling up SOA to learn better features. We believe that the challenges presented by SOA offer the opportunity for further advancement in video analysis as we progress from single-label classification towards a more comprehensive understanding of video data.",
isbn="978-3-030-01264-9"
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@InProceedings{marszalek09,
    author = "Marcin Marsza{\l}ek and Ivan Laptev and Cordelia Schmid",
    title = "Actions in Context",
    booktitle = "IEEE Conference on Computer Vision \& Pattern Recognition",
    year = "2009"
}

@inproceedings{moviegraphs,
  title={MovieGraphs: Towards Understanding Human-Centric Situations from Videos},
  author={Paul Vicol and Makarand Tapaswi and Lluis Castrejon and Sanja Fidler},
  booktitle={{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year={2018}
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@inproceedings{lvu2021,
  Author    = {Chao-Yuan Wu and Philipp Kr\"{a}henb\"{u}hl},
  Title     = {{Towards Long-Form Video Understanding}},
  Booktitle = {{CVPR}},
  Year      = {2021}}


@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@INPROCEEDINGS{BayesianFeiFeiLi,
  author={Fei-Fei, L. and Perona, P.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={A Bayesian hierarchical model for learning natural scene categories}, 
  year={2005},
  volume={2},
  number={},
  pages={524-531 vol. 2},
  doi={10.1109/CVPR.2005.16}}

@InProceedings{sariyildiz2020icmlm,
author = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
title = {Learning Visual Representations with Caption Annotations},
booktitle = {European Conference on Computer Vision (ECCV)},
year = {2020}
}

@inproceedings{desai2021virtex,
    title={{VirTex: Learning Visual Representations from Textual Annotations}},
    author={Karan Desai and Justin Johnson},
    booktitle={CVPR},
    year={2021}
}

@article{Shen2021HowMC,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Sheng Shen and Liunian Harold Li and Hao Tan and Mohit Bansal and Anna Rohrbach and Kai-Wei Chang and Zhewei Yao and Kurt Keutzer},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.06383}
}

@inproceedings{Gu2021OpenvocabularyOD,
  title={Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},
  author={Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},
  year={2021}
}

@article{Li2022LanguagedrivenSS,
  title={Language-driven Semantic Segmentation},
  author={Boyi Li and Kilian Q. Weinberger and Serge J. Belongie and Vladlen Koltun and Ren{\'e} Ranftl},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03546}
}

 @InProceedings{10.1007/978-3-540-88693-8_12,
author="Cour, Timothee
and Jordan, Chris
and Miltsakaki, Eleni
and Taskar, Ben",
editor="Forsyth, David
and Torr, Philip
and Zisserman, Andrew",
title="Movie/Script: Alignment and Parsing of Video and Text Transcription",
booktitle="Computer Vision -- ECCV 2008",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="158--171",
abstract="Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales ``in the wild''. Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series.",
isbn="978-3-540-88693-8"
}

@inproceedings{Everingham2006HelloMN,
  title={Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video},
  author={Mark Everingham and Josef Sivic and Andrew Zisserman},
  booktitle={BMVC},
  year={2006}
}

@article{Rohrbach2015ADF,
  title={A dataset for Movie Description},
  author={Anna Rohrbach and Marcus Rohrbach and Niket Tandon and Bernt Schiele},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={3202-3212}
}

@INPROCEEDINGS{Laptevactionscvpr,
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Learning realistic human actions from movies}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/CVPR.2008.4587756}}
