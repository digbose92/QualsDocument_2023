%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for ZHANG Ruda at 2016-04-04 14:14:11 -0700


%% Saved with string encoding Unicode (UTF-8)


@article{einstein_ist_1905,
	title = {Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?},
	volume = {323},
	rights = {Copyright © 1905 {WILEY}‐{VCH} Verlag {GmbH} \& Co. {KGaA}, Weinheim},
	issn = {1521-3889},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/andp.19053231314},
	doi = {10.1002/andp.19053231314},
	pages = {639--641},
	number = {13},
	journaltitle = {Annalen der Physik},
	author = {Einstein, A.},
	date = {1905}
}

@book{Fisher:1954,
	Address = {New York},
	Author = {Ronald Aylmer Fisher},
	Edition = {12th},
	Publisher = {Hafner},
	Title = {Statistical methods for research workers},
	Year = {1954}}

@article{Robbins:1951aa,
	Author = {Robbins, Herbert and Sutton Monro},
	Journal = {The Annals of Mathematical Statistics},
	Number = {3},
	Pages = {pp. 400-407},
	Title = {A Stochastic Approximation Method},
	Volume = {22},
	Year = {1951}}

@book{Knight:1921,
	Address = {Boston, New York},
	Author = {Frank H Knight},
	Publisher = {Houghton Mifflin Company},
	Title = {Risk, uncertainty and profit},
	Year = {1921}}

@article{Wright:1921aa,
	Author = {Wright, Sewall},
	Journal = {Journal of agricultural research},
	Pages = {557-585},
	Title = {Correlation and Causation},
	Year = {1921}}

@article{Caratheodory:1909aa,
	Author = {Constantin Carath{\'e}odory},
	Journal = {Mathematische Annalen},
	Pages = {355--386},
	Title = {Untersuchungen {\"u}ber die Grundlagen der Thermodynamik},
	Volume = {67},
	Year = {1909}}

@book{Gibbs:1902,
	Address = {New York},
	Author = {Gibbs, J Willard},
	Publisher = {C. Scribner's sons},
	Title = {Elementary principles in statistical mechanics: Developed with especial reference to the rational foundations of thermodynamics},
	Year = {1902}}

@article{Clausius:1857,
	Author = {Clausius, Rudolf},
	Journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	Number = {91},
	Pages = {108--127},
	Title = {XI. On the nature of the motion which we call heat},
	Volume = {14},
	Year = {1857}}

@article{contextvision,
author = {Wang, Xuan and Zhu, Zhigang},
title = {Context Understanding in Computer Vision: A Survey},
year = {2023},
issue_date = {Mar 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {229},
number = {C},
issn = {1077-3142},
url = {https://doi-org.libproxy2.usc.edu/10.1016/j.cviu.2023.103646},
doi = {10.1016/j.cviu.2023.103646},
journal = {Comput. Vis. Image Underst.},
month = {apr},
numpages = {28},
keywords = {65D17, Deep Learning Models, Context, 41A10, 41A05, Computer Vision, Context Integration, 65D05}
}

@article{Rabinovich2007ObjectsIC,
  title={Objects in Context},
  author={Andrew Rabinovich and Andrea Vedaldi and Carolina Galleguillos and Eric Wiewiora and Serge J. Belongie},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:749550}
}

@article{Wang2007ShapeAA,
  title={Shape and Appearance Context Modeling},
  author={Xiaogang Wang and Gianfranco Doretto and Thomas B. Sebastian and J. Rittscher and Peter H. Tu},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:14958192}
}

@ARTICLE {Faceness-Net,
author = {S. Yang and P. Luo and C. Loy and X. Tang},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Faceness-Net: Face Detection through Deep Facial Part Responses},
year = {2018},
volume = {40},
number = {08},
issn = {1939-3539},
pages = {1845-1859},
abstract = {We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.},
keywords = {face;proposals;face detection;detectors;neural networks;mouth;training},
doi = {10.1109/TPAMI.2017.2738644},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@article{Marques2010ContextMI,
  title={Context modeling in computer vision: techniques, implications, and applications},
  author={Oge Marques and Elan Barenholtz and Vincent Charvillat},
  journal={Multimedia Tools and Applications},
  year={2010},
  volume={51},
  pages={303-339},
  url={https://api.semanticscholar.org/CorpusID:18206222}
}

@article{Chang2021ACS,
  title={A Comprehensive Survey of Scene Graphs: Generation and Application},
  author={Xiaojun Chang and Pengzhen Ren and Pengfei Xu and Zhihui Li and Xiaojiang Chen and Alexander G. Hauptmann},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={45},
  pages={1-26},
  url={https://api.semanticscholar.org/CorpusID:245445853}
}

@article{Xu2017SceneGG,
  title={Scene Graph Generation by Iterative Message Passing},
  author={Danfei Xu and Yuke Zhu and Christopher Bongsoo Choy and Li Fei-Fei},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={3097-3106},
  url={https://api.semanticscholar.org/CorpusID:1780254}
}

@article{Yan2019LearningCG,
  title={Learning Context Graph for Person Search},
  author={Yichao Yan and Qiang Zhang and Bingbing Ni and Wendong Zhang and Minghao Xu and Xiaokang Yang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2153-2162},
  url={https://api.semanticscholar.org/CorpusID:102487416}
}

@article{Carreira2017QuoVA,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733},
  url={https://api.semanticscholar.org/CorpusID:206596127}
}

@article{Ji2021DetectingHR,
  title={Detecting Human-Object Relationships in Videos},
  author={Jingwei Ji and Rishi Desai and Juan Carlos Niebles},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={8086-8096},
  url={https://api.semanticscholar.org/CorpusID:242213105}
}

@article{Wu2021TowardsLV,
  title={Towards Long-Form Video Understanding},
  author={Chaoxia Wu and Philipp Kr{\"a}henb{\"u}hl},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={1884-1894},
  url={https://api.semanticscholar.org/CorpusID:235416690}
}

@article{Soldan2021MADAS,
  title={MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},
  author={Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={5016-5025},
  url={https://api.semanticscholar.org/CorpusID:244773187}
}

@article{Beery2019ContextRL,
  title={Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection},
  author={Sara Beery and Guanhang Wu and Vivek Rathod and Ronny Votel and Jonathan Huang},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={13072-13082},
  url={https://api.semanticscholar.org/CorpusID:208921095}
}

@article{purushwalkam2020audio,
  title={Audio-visual floorplan reconstruction},
  author={Purushwalkam, Senthil and Gari, Sebastian Vicenc Amengual and Ithapu, Vamsi Krishna and Schissler, Carl and Robinson, Philip and Gupta, Abhinav and Grauman, Kristen},
  journal={arXiv preprint arXiv:2012.15470},
  year={2020}
}

@inproceedings{chen2020soundspaces,
    title = {SoundSpaces: Audio-Visual Navigation in 3D Environments},
    author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
    year = {2020},
    booktitle={ECCV},
}

@article{Tian2018AudioVisualEL,
  title={Audio-Visual Event Localization in Unconstrained Videos},
  author={Yapeng Tian and Jing Shi and Bochen Li and Zhiyao Duan and Chenliang Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08842},
  url={https://api.semanticscholar.org/CorpusID:4336836}
}

@article{Zhang2022TemporalSG,
  title={Temporal Sentence Grounding in Videos: A Survey and Future Directions},
  author={Hao Zhang and Aixin Sun and Wei Jing and Joey Tianyi Zhou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  volume={45},
  pages={10443-10465},
  url={https://api.semanticscholar.org/CorpusID:253224445}
}

@article{Seymour2017AutomatedDA,
  title={Automated detection and enumeration of marine wildlife using unmanned aircraft systems (UAS) and thermal imagery},
  author={Alexander C. Seymour and Julian Dale and Mike O. Hammill and Patrick N. Halpin and David W. Johnston},
  journal={Scientific Reports},
  year={2017},
  volume={7},
  url={https://api.semanticscholar.org/CorpusID:14140341}
}

@article{Baltruaitis2017MultimodalML,
  title={Multimodal Machine Learning: A Survey and Taxonomy},
  author={Tadas Baltru{\vs}aitis and Chaitanya Ahuja and Louis-Philippe Morency},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  volume={41},
  pages={423-443},
  url={https://api.semanticscholar.org/CorpusID:10137425}
}

@article{Gao2020ASO,
  title={A Survey on Deep Learning for Multimodal Data Fusion},
  author={Jing Gao and Peng Li and Zhikui Chen and Jianing Zhang},
  journal={Neural Computation},
  year={2020},
  volume={32},
  pages={829-864},
  url={https://api.semanticscholar.org/CorpusID:212748233}
}

@article{Abdar2023ARO,
  title={A Review of Deep Learning for Video Captioning},
  author={Moloud Abdar and Meenakshi Kollati and Swaraja Kuraparthi and Farhad Pourpanah and Daniel J. McDuff and Mohammad Ghavamzadeh and Shuicheng Yan and Abduallah A. Mohamed and Abbas Khosravi and E. Cambria and Fatih Murat Porikli},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11431},
  url={https://api.semanticscholar.org/CorpusID:258298547}
}

@article{Apostolidis2021VideoSU,
  title={Video Summarization Using Deep Neural Networks: A Survey},
  author={Evlampios Apostolidis and E. Adamantidou and Alexandros I. Metsai and Vasileios Mezaris and I. Patras},
  journal={Proceedings of the IEEE},
  year={2021},
  volume={109},
  pages={1838-1863},
  url={https://api.semanticscholar.org/CorpusID:231627658}
}

@misc{defaria2023visual,
      title={Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature}, 
      author={Ana Cláudia Akemi Matsuki de Faria and Felype de Castro Bastos and José Victor Nogueira Alves da Silva and Vitor Lopes Fabris and Valeska de Sousa Uchoa and Décio Gonçalves de Aguiar Neto and Claudio Filipi Goncalves dos Santos},
      year={2023},
      eprint={2305.11033},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Alain2001WhatA,
  title={“What” and “where” in the human auditory system},
  author={Claude Alain and Stephen R. Arnott and Stephanie J. Hevenor and Stephen Graham and Cheryl L. Grady},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2001},
  volume={98},
  pages={12301 - 12306},
  url={https://api.semanticscholar.org/CorpusID:25136641}
}

@article{BornkesselSchlesewsky2015NeurobiologicalRO,
  title={Neurobiological roots of language in primate audition: common computational properties},
  author={Ina Bornkessel-Schlesewsky and Matthias Schlesewsky and Steven L. Small and Josef P. Rauschecker},
  journal={Trends in Cognitive Sciences},
  year={2015},
  volume={19},
  pages={142-150},
  url={https://api.semanticscholar.org/CorpusID:11071265}
}

@article{Wallace2002HistochemicalIO,
  title={Histochemical identification of cortical areas in the auditory region of the human brain},
  author={Mark N. Wallace and Peter W. Johnston and Alan R. Palmer},
  journal={Experimental Brain Research},
  year={2002},
  volume={143},
  pages={499-508},
  url={https://api.semanticscholar.org/CorpusID:24211906}
}

@article{Torralba2003ContextualPF,
  title={Contextual Priming for Object Detection},
  author={Antonio Torralba},
  journal={International Journal of Computer Vision},
  year={2003},
  volume={53},
  pages={169-191},
  url={https://api.semanticscholar.org/CorpusID:1073705}
}

@article{Choi2012ContextMA,
  title={Context models and out-of-context objects},
  author={Myung Jin Choi and Antonio Torralba and Alan S. Willsky},
  journal={Pattern Recognit. Lett.},
  year={2012},
  volume={33},
  pages={853-862},
  url={https://api.semanticscholar.org/CorpusID:8354810}
}

@article{barrettcontext,
author = {Lisa Feldman Barrett and Batja Mesquita and Maria Gendron},
title ={Context in Emotion Perception},
journal = {Current Directions in Psychological Science},
volume = {20},
number = {5},
pages = {286-290},
year = {2011},
doi = {10.1177/0963721411422522},

URL = { 
        https://doi.org/10.1177/0963721411422522
    
},
eprint = { 
        https://doi.org/10.1177/0963721411422522
    
}
,
    abstract = { We review recent work demonstrating consistent context effects during emotion perception. Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotion is perceived in a face, calling into question the still-common assumption that the emotional state of a person is written on and can be read from the face like words on a page. Incorporating context during emotion perception appears to be routine, efficient, and, to some degree, automatic. This evidence challenges the standard view of emotion perception represented in psychology texts, in the cognitive neuroscience literature, and in the popular media and points to a necessary change in the basic paradigm used in the scientific study of emotion perception. }
}

@inproceedings{Zhao2022M3EDMM,
  title={M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database},
  author={Jinming Zhao and Tenggan Zhang and Jingwen Hu and Yuchen Liu and Qin Jin and Xinchao Wang and Haizhou Li},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248780564}
}

@article{Das2016VisualD,
  title={Visual Dialog},
  author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos{\'e} M. F. Moura and Devi Parikh and Dhruv Batra},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1080-1089},
  url={https://api.semanticscholar.org/CorpusID:1820614}
}

@ARTICLE{CMI,
  author={Somandepalli, Krishna and Guha, Tanaya and Martinez, Victor R. and Kumar, Naveen and Adam, Hartwig and Narayanan, Shrikanth},
  journal={Proceedings of the IEEE}, 
  title={Computational Media Intelligence: Human-Centered Machine Analysis of Media}, 
  year={2021},
  volume={109},
  number={5},
  pages={891-910},
  doi={10.1109/JPROC.2020.3047978}}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231591445}
}

@article{Bar2004VisualOI,
  title={Visual objects in context},
  author={Moshe Bar},
  journal={Nature Reviews Neuroscience},
  year={2004},
  volume={5},
  pages={617-629},
  url={https://api.semanticscholar.org/CorpusID:205499985}
}

@article{Qiao2021ObjectLevelSC,
  title={Object-Level Scene Context Prediction},
  author={Xiaotian Qiao and Quanlong Zheng and Ying Cao and Rynson W. H. Lau},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={44},
  pages={5280-5292},
  url={https://api.semanticscholar.org/CorpusID:233428370}
}

@inproceedings{Bordwell1979FilmAA,
  title={Film Art: An Introduction},
  author={David Bordwell and Kristin Thompson},
  year={1979},
  url={https://api.semanticscholar.org/CorpusID:190106329}
}

@article{Xiao2010SUNDL,
  title={SUN database: Large-scale scene recognition from abbey to zoo},
  author={Jianxiong Xiao and James Hays and Krista A. Ehinger and Aude Oliva and Antonio Torralba},
  journal={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year={2010},
  pages={3485-3492},
  url={https://api.semanticscholar.org/CorpusID:1309931}
}

@article{zhou2017places,
  title={Places: A 10 million Image Database for Scene Recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@misc{bain2020condensed,
    title={Condensed Movies: Story Based Retrieval with Contextual Embeddings},
    author={Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman},
    year={2020},
    eprint={2005.04208},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{Sadhu_2021_CVPR,
          author = {Sadhu, Arka and Gupta, Tanmay and Yatskar, Mark and Nevatia, Ram and Kembhavi, Aniruddha},
          title = {Visual Semantic Role Labeling for Video Understanding},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
          month = {June},
          year = {2021}}


@InProceedings{SBD,
author="Helm, Daniel
and Kampel, Martin",
editor="Cristani, Marco
and Prati, Andrea
and Lanz, Oswald
and Messelodi, Stefano
and Sebe, Nicu",
title="Shot Boundary Detection for Automatic Video Analysis of Historical Films",
booktitle="New Trends in Image Analysis and Processing -- ICIAP 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="137--147",
abstract="In automatic video content analysis and film preservation, Shot Boundary Detection (SBD) is a fundamental pre-processing step. While previous research focuses on detecting Abrupt Transitions (AT) as well as Gradual Transitions (GT) in different video genres such as sports movies or news clips only few studies investigate in the detection of shot transitions in historical footage. The main aim of this paper is to create an SBD mechanism inspired by state-of-the-art algorithms which is applied and evaluated on a self-generated historical dataset as well as a publicly available dataset called Clipshots. Therefore, a three-stage pipeline is introduced consisting of a Candidate Frame Range Selection based on the network DeepSBD, Extraction of Convolutional Neural Network (CNN) Features and Similarity Calculation. A combination of pre-trained backbone CNNs such as ResNet, VGG19 and SqueezeNet with different similarity metrics like Cosine Similarity and Euclidean Distance are used and evaluated. The outcome of this paper displays that the proposed algorithm reaches promising results on detecting ATs in historical videos without the need of complex optimization and re-training processes. Furthermore, it points out the main challenges concerning historical footage such as damaged film reels, scratches or splices. The results of this paper contribute a significant base for future research on automatic video analysis of historical videos.",
isbn="978-3-030-30754-7"
}

@article{CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               others},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {CoRR},
  volume    = {abs/2103.00020},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  eprinttype = {arXiv},
  eprint    = {2103.00020},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@article{2019Moviescope,
title={Moviescope: Large-scale Analysis of Movies using Multiple Modalities},
author={Paola Cascante-Bonilla and Kalpathy Sitaraman and Mengjia Luo and Vicente Ordonez},
journal={ArXiv},
year={2019},
volume={abs/1908.03180}
}

@INPROCEEDINGS{IndoorScenes,
  author={Quattoni, Ariadna and Torralba, Antonio},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Recognizing indoor scenes}, 
  year={2009},
  volume={},
  number={},
  pages={413-420},
  doi={10.1109/CVPR.2009.5206537}}

@article{xiao_sun_2016,
	title = {{SUN} {Database}: {Exploring} a {Large} {Collection} of {Scene} {Categories}},
	volume = {119},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0748-y},
	doi = {10.1007/s11263-014-0748-y},
	abstract = {Progress in scene understanding requires reasoning about the rich and diverse visual environments that make up our daily experience. To this end, we propose the Scene Understanding database, a nearly exhaustive collection of scenes categorized at the same level of specificity as human discourse. The database contains 908 distinct scene categories and 131,072 images. Given this data with both scene and object labels available, we perform in-depth analysis of co-occurrence statistics and the contextual relationship. To better understand this large scale taxonomy of scene categories, we perform two human experiments: we quantify human scene recognition accuracy, and we measure how typical each image is of its assigned scene category. Next, we perform computational experiments: scene recognition with global image features, indoor versus outdoor classification, and “scene detection,” in which we relax the assumption that one image depicts only one scene category. Finally, we relate human experiments to machine performance and explore the relationship between human and machine recognition errors and the relationship between image “typicality” and machine recognition accuracy.},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Xiao, Jianxiong and Ehinger, Krista A. and Hays, James and Torralba, Antonio and Oliva, Aude},
	month = aug,
	year = {2016},
	pages = {3--22},
}

@article{kinetics400,
  author    = {Will Kay and
               Jo{\~{a}}o Carreira and
               Karen Simonyan and
               Brian Zhang and
               Chloe Hillier and
               Sudheendra Vijayanarasimhan and
               Fabio Viola and
               Tim Green and
               Trevor Back and
               Paul Natsev and
               Mustafa Suleyman and
               Andrew Zisserman},
  title     = {The Kinetics Human Action Video Dataset},
  journal   = {CoRR},
  volume    = {abs/1705.06950},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06950},
  eprinttype = {arXiv},
  eprint    = {1705.06950},
  timestamp = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KayCSZHVVGBNSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@article{Kinetics700,
  author    = {Jo{\~{a}}o Carreira and
               Eric Noland and
               Chloe Hillier and
               Andrew Zisserman},
  title     = {A Short Note on the Kinetics-700 Human Action Dataset},
  journal   = {CoRR},
  volume    = {abs/1907.06987},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.06987},
  eprinttype = {arXiv},
  eprint    = {1907.06987},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-06987.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6047--6056},
  year={2018}
}
  
@article{Something-SomethingV2,
  author    = {Raghav Goyal and
               Samira Ebrahimi Kahou and
               Vincent Michalski and
               Joanna Materzynska and
               Susanne Westphal and others},
  title     = {The "something something" video database for learning and evaluating
               visual common sense},
  journal   = {CoRR},
  volume    = {abs/1706.04261},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04261},
  eprinttype = {arXiv},
  eprint    = {1706.04261},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalKMMWKHFYMH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{SOA,
author="Ray, Jamie
and Wang, Heng
and Tran, Du
and Wang, Yufei
and Feiszli, Matt
and Torresani, Lorenzo
and Paluri, Manohar",
title="Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="660--676",
abstract="This paper introduces a large-scale, multi-label and multi-task video dataset named Scenes-Objects-Actions (SOA). Most prior video datasets are based on a predefined taxonomy, which is used to define the keyword queries issued to search engines. The videos retrieved by the search engines are then verified for correctness by human annotators. Datasets collected in this manner tend to generate high classification accuracy as search engines typically rank ``easy'' videos first. The SOA dataset adopts a different approach. We rely on uniform sampling to get a better representation of videos on the Web. Trained annotators are asked to provide free-form text labels describing each video in three different aspects: scene, object and action. These raw labels are then merged, split and renamed to generate a taxonomy for SOA. All the annotations are verified again based on the taxonomy. The final dataset includes 562K videos with 3.64M annotations spanning 49 categories for scenes, 356 for objects, 148 for actions, and naturally captures the long tail distribution of visual concepts in the real world. We show that datasets collected in this way are quite challenging by evaluating existing popular video models on SOA. We provide in-depth analysis about the performance of different models on SOA, and highlight potential new directions in video classification. We compare SOA with existing datasets and discuss various factors that impact the performance of transfer learning. A key-feature of SOA is that it enables the empirical study of correlation among scene, object and action recognition in video. We present results of this study and further analyze the potential of using the information learned from one task to improve the others. We also demonstrate different ways of scaling up SOA to learn better features. We believe that the challenges presented by SOA offer the opportunity for further advancement in video analysis as we progress from single-label classification towards a more comprehensive understanding of video data.",
isbn="978-3-030-01264-9"
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@InProceedings{marszalek09,
    author = "Marcin Marsza{\l}ek and Ivan Laptev and Cordelia Schmid",
    title = "Actions in Context",
    booktitle = "IEEE Conference on Computer Vision \& Pattern Recognition",
    year = "2009"
}

@inproceedings{moviegraphs,
  title={MovieGraphs: Towards Understanding Human-Centric Situations from Videos},
  author={Paul Vicol and Makarand Tapaswi and Lluis Castrejon and Sanja Fidler},
  booktitle={{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year={2018}
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@inproceedings{lvu2021,
  Author    = {Chao-Yuan Wu and Philipp Kr\"{a}henb\"{u}hl},
  Title     = {{Towards Long-Form Video Understanding}},
  Booktitle = {{CVPR}},
  Year      = {2021}}


@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@INPROCEEDINGS{BayesianFeiFeiLi,
  author={Fei-Fei, L. and Perona, P.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={A Bayesian hierarchical model for learning natural scene categories}, 
  year={2005},
  volume={2},
  number={},
  pages={524-531 vol. 2},
  doi={10.1109/CVPR.2005.16}}

@InProceedings{sariyildiz2020icmlm,
author = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
title = {Learning Visual Representations with Caption Annotations},
booktitle = {European Conference on Computer Vision (ECCV)},
year = {2020}
}

@inproceedings{desai2021virtex,
    title={{VirTex: Learning Visual Representations from Textual Annotations}},
    author={Karan Desai and Justin Johnson},
    booktitle={CVPR},
    year={2021}
}

@article{Shen2021HowMC,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Sheng Shen and Liunian Harold Li and Hao Tan and Mohit Bansal and Anna Rohrbach and Kai-Wei Chang and Zhewei Yao and Kurt Keutzer},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.06383}
}

@inproceedings{Gu2021OpenvocabularyOD,
  title={Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},
  author={Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},
  year={2021}
}

@article{Li2022LanguagedrivenSS,
  title={Language-driven Semantic Segmentation},
  author={Boyi Li and Kilian Q. Weinberger and Serge J. Belongie and Vladlen Koltun and Ren{\'e} Ranftl},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03546}
}

 @InProceedings{10.1007/978-3-540-88693-8_12,
author="Cour, Timothee
and Jordan, Chris
and Miltsakaki, Eleni
and Taskar, Ben",
editor="Forsyth, David
and Torr, Philip
and Zisserman, Andrew",
title="Movie/Script: Alignment and Parsing of Video and Text Transcription",
booktitle="Computer Vision -- ECCV 2008",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="158--171",
abstract="Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales ``in the wild''. Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series.",
isbn="978-3-540-88693-8"
}

@inproceedings{Everingham2006HelloMN,
  title={Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video},
  author={Mark Everingham and Josef Sivic and Andrew Zisserman},
  booktitle={BMVC},
  year={2006}
}

@article{Rohrbach2015ADF,
  title={A dataset for Movie Description},
  author={Anna Rohrbach and Marcus Rohrbach and Niket Tandon and Bernt Schiele},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={3202-3212}
}

@INPROCEEDINGS{Laptevactionscvpr,
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Learning realistic human actions from movies}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/CVPR.2008.4587756}}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@article{brendanfrey,
author = {Brendan J. Frey  and Delbert Dueck},
title = {Clustering by Passing Messages Between Data Points},
journal = {Science},
volume = {315},
number = {5814},
pages = {972-976},
year = {2007},
doi = {10.1126/science.1136800},
URL = {https://www.science.org/doi/abs/10.1126/science.1136800},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1136800},
abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such “exemplars” can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.}}


@article{tSNE,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{BarsoumICMI2016,
    title={Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
    author={Barsoum, Emad and Zhang, Cha and Canton Ferrer, Cristian and Zhang, Zhengyou},
    booktitle={ACM International Conference on Multimodal Interaction (ICMI)},
    year={2016}
}

@article{Deng2009ImageNetAL,
  title={ImageNet: A large-scale hierarchical image database},
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  pages={248-255}
}

@InProceedings{parkhi12a,
  author       = "Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}
@inproceedings{bossard14,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}

@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,
  title = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  address = {Sydney, Australia},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}

@article{Deng2009ImageNetAL,
  title={ImageNet: A large-scale hierarchical image database},
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  pages={248-255}
}

@article{FeiFei2004LearningGV,
  title={Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},
  author={Li Fei-Fei and Rob Fergus and Pietro Perona},
  journal={Computer Vision and Pattern Recognition Workshop},
  year={2004},
}

@article{ucf101,
  author    = {Khurram Soomro and
               Amir Roshan Zamir and
               Mubarak Shah},
  title     = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
               The Wild},
  journal   = {CoRR},
  volume    = {abs/1212.0402},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.0402},
  eprinttype = {arXiv},
  eprint    = {1212.0402},
  timestamp = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{rao2020unified,
title={A Unified Framework for Shot Type Classification Based on Subject Centric Lens},
author={Rao, Anyi and Wang, Jiaze and Xu, Linning and Jiang, Xuekun and Huang, Qingqiu and Zhou, Bolei and Lin, Dahua},
booktitle = {The European Conference on Computer Vision (ECCV)}, 
year={2020}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
  }

@article{lstm,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@incollection{Pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and others},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{i3d,
  author = {Carreira, J. and Zisserman, Andrew},
  year = {2017},
  month = {07},
  pages = {4724-4733},
  title = {Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  doi = {10.1109/CVPR.2017.502}
}
@inproceedings{r2plus1d,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={6450--6459},
  year={2018}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={6202--6211},
  year={2019}
}

@article{Bertasius2021IsSA,
  title={Is Space-Time Attention All You Need for Video Understanding?},
  author={Gedas Bertasius and Heng Wang and Lorenzo Torresani},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.05095}
}

@article{liu2021video,
  title={Video Swin Transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}

@article{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}

@ARTICLE{DuTran,
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
title = {Learning Spatiotemporal Features with 3D Convolutional Networks},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
year = 2014,
month = dec,
eid = {arXiv:1412.0767}
}

@article{He2015,
	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title = {Deep Residual Learning for Image Recognition},
	journal = {arXiv preprint arXiv:1512.03385},
	year = {2015}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}  

@article{dukes2021,
  title={The rise of affectivism},
  author={Dukes, Daniel and Abrams, Kathryn and Adolphs, Ralph and Ahmed, Mohammed E and Beatty, Andrew and Berridge, Kent C and others},
  journal={Nature human behaviour},
  volume={5},
  number={7},
  pages={816--820},
  year={2021},
  publisher={Nature Publishing Group}
}

@ARTICLE{AICA,
  author={Zhao, Sicheng and Yao, Xingxu and Yang, Jufeng and Jia, Guoli and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Affective Image Content Analysis: Two Decades Review and New Perspectives}, 
  year={2022},
  volume={44},
  number={10},
  pages={6729-6751},
  doi={10.1109/TPAMI.2021.3094362}}

@ARTICLE {speechemo,
author = {S. Latif and R. Rana and S. Khalifa and R. Jurdak and J. Qadir and B. W. Schuller},
journal = {IEEE Transactions on Affective Computing},
title = {Survey of Deep Representation Learning for Speech Emotion Recognition},
year = {5555},
volume = {},
number = {01},
issn = {1949-3045},
pages = {1-1},
abstract = {Traditionally, speech emotion recognition (SER) research has relied on manually handcrafted acoustic features using feature engineering. However, the design of handcrafted features for complex SER tasks requires significant manual eort, which impedes generalisability and slows the pace of innovation. This has motivated the adoption of representation learning techniques that can automatically learn an intermediate representation of the input signal without any manual feature engineering. Representation learning has led to improved SER performance and enabled rapid innovation. Its effectiveness has further increased with advances in deep learning (DL), which has facilitated \textit{deep representation learning} where hierarchical representations are automatically learned in a data-driven manner. This paper presents the first comprehensive survey on the important topic of deep representation learning for SER. We highlight various techniques, related challenges and identify important future areas of research. Our survey bridges the gap in the literature since existing surveys either focus on SER with hand-engineered features or representation learning in the general setting without focusing on SER.},
keywords = {principal component analysis;task analysis;speech recognition;emotion recognition;deep learning;australia;neurons},
doi = {10.1109/TAFFC.2021.3114365},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep}
}

@article{Poria2019EmotionRI,
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances},
  author={Soujanya Poria and Navonil Majumder and Rada Mihalcea and Eduard H. Hovy},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={100943-100953},
  url={https://api.semanticscholar.org/CorpusID:147703962}
}

@INPROCEEDINGS{depressiondetection,
  author={Zucco, Chiara and Calabrese, Barbara and Cannataro, Mario},
  booktitle={2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Sentiment analysis and affective computing for depression monitoring}, 
  year={2017},
  volume={},
  number={},
  pages={1988-1995},
  doi={10.1109/BIBM.2017.8217966}}

@ARTICLE{autismguha,
  author={Guha, Tanaya and Yang, Zhaojun and Grossman, Ruth B. and Narayanan, Shrikanth S.},
  journal={IEEE Transactions on Affective Computing}, 
  title={A Computational Study of Expressive Facial Dynamics in Children with Autism}, 
  year={2018},
  volume={9},
  number={1},
  pages={14-20},
  doi={10.1109/TAFFC.2016.2578316}}

@ARTICLE{savchecnkoengagement,
  author={Savchenko, Andrey V. and Savchenko, Lyudmila V. and Makarov, Ilya},
  journal={IEEE Transactions on Affective Computing}, 
  title={Classifying emotions and engagement in online learning based on a single facial expression recognition neural network}, 
  year={2022},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TAFFC.2022.3188390}}

@inproceedings{DFEW,
author = {Jiang, Xingxun and Zong, Yuan and others},
title = {DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413620},
doi = {10.1145/3394171.3413620},
abstract = {Recently, facial expression recognition (FER) in the wild has gained a lot of researchers' attention because it is a valuable topic to enable the FER techniques to move from the laboratory to the real applications. In this paper, we focus on this challenging but interesting topic and make contributions from three aspects. First, we present a new large-scale 'in-the-wild' dynamic facial expression database, DFEW (Dynamic Facial Expression in the Wild), consisting of over 16,000 video clips from thousands of movies. These video clips contain various challenging interferences in practical scenarios such as extreme illumination, occlusions, and capricious pose changes. Second, we propose a novel method called Expression-Clustered Spatiotemporal Feature Learning (EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct extensive benchmark experiments on DFEW using a lot of spatiotemporal deep feature learning methods as well as our proposed EC-STFL. Experimental results show that DFEW is a well-designed and challenging database, and the proposed EC-STFL can promisingly improve the performance of existing spatiotemporal deep neural networks in coping with the problem of dynamic FER in the wild. Our DFEW database is publicly available and can be freely downloaded from https://dfew-dataset.github.io/.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2881–2889},
numpages = {9},
keywords = {in-the-wild facial expression recognition, dynamic facial expression, facial expression database, deep learning},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{Mollahosseini2019AffectNetAD,
  title={AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild},
  author={Ali Mollahosseini and Behzad Hasani and Mohammad H. Mahoor},
  journal={IEEE Transactions on Affective Computing},
  year={2019},
  volume={10},
  pages={18-31}
}


@article{BarretEmotionPerception,
author = {Lisa Feldman Barrett and Batja Mesquita and Maria Gendron},
title ={Context in Emotion Perception},
journal = {Current Directions in Psychological Science},
volume = {20},
number = {5},
pages = {286-290},
year = {2011},
doi = {10.1177/0963721411422522},

URL = { 
        https://doi.org/10.1177/0963721411422522
    
},
eprint = { 
        https://doi.org/10.1177/0963721411422522
    
}
,
    abstract = { We review recent work demonstrating consistent context effects during emotion perception. Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotion is perceived in a face, calling into question the still-common assumption that the emotional state of a person is written on and can be read from the face like words on a page. Incorporating context during emotion perception appears to be routine, efficient, and, to some degree, automatic. This evidence challenges the standard view of emotion perception represented in psychology texts, in the cognitive neuroscience literature, and in the popular media and points to a necessary change in the basic paradigm used in the scientific study of emotion perception. }
}


@article{Bar2004VisualOI,
  title={Visual objects in context},
  author={Moshe Bar},
  journal={Nature Reviews Neuroscience},
  year={2004},
  volume={5},
  pages={617-629}
}

@inproceedings{cho2021vlt5,
  title     = {Unifying Vision-and-Language Tasks via Text Generation},
  author    = {Jaemin Cho and Jie Lei and Hao Tan and Mohit Bansal},
  booktitle = {ICML},
  year      = {2021}
}

@article{wang2022ofa,
  author    = {Peng Wang and
               An Yang and
               Rui Men and
               Junyang Lin and others},
  title     = {OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence
               Learning Framework},
  journal   = {CoRR},
  volume    = {abs/2202.03052},
  year      = {2022}
  
 }
 
 @article{Masuda2008PlacingTF,
  title={Placing the face in context: cultural differences in the perception of facial emotion.},
  author={Takahiko Masuda and Phoebe C. Ellsworth and Batja Mesquita and others},
  journal={Journal of personality and social psychology},
  year={2008},
  volume={94 3},
  pages={
          365-81
        }
}

@ARTICLE{AffectNet,
  author={Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
  journal={IEEE Transactions on Affective Computing}, 
  title={AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild}, 
  year={2019},
  volume={10},
  number={1},
  pages={18-31},
  doi={10.1109/TAFFC.2017.2740923}}
  
@inproceedings{BarsoumICMI2016,
    title={Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
    author={Barsoum, Emad and Zhang, Cha and Canton Ferrer, Cristian and Zhang, Zhengyou},
    booktitle={ACM International Conference on Multimodal Interaction (ICMI)},
    year={2016}
}

@article{Luo2019ARBEETA,
  title={ARBEE: Towards Automated Recognition of Bodily Expression of Emotion in the Wild},
  author={Yu Luo and Jianbo Ye and Reginald B. Adams and others},
  journal={International Journal of Computer Vision},
  year={2019},
  volume={128},
  pages={1-25}
}

@inproceedings{pikoulis2021leveraging,
  author={Pikoulis, Ioannis and Filntisis, Panagiotis P. and Maragos, Petros},
  booktitle={2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)}, 
  title={Leveraging Semantic Scene Characteristics and Multi-Stream Convolutional Architectures in a Contextual Approach for Video-Based Visual Emotion Recognition in the Wild}, 
  year={2021},
  volume={},
  number={},
  doi={10.1109/FG52635.2021.9666957}
}

@article{Mittal2020EmotiConCM,
  title={EmotiCon: Context-Aware Multimodal Emotion Recognition Using Frege’s Principle},
  author={Trisha Mittal and Pooja Guhan and others},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2020},
  pages={14222-14231}
}

@article{Dudzik2019ContextIH,
  title={Context in Human Emotion Perception for Automatic Affect Detection: A Survey of Audiovisual Databases},
  author={Bernd Dudzik and Michel-Pierre Jansen and Franziska Burger and Frank Kaptein and others},
  journal={2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)},
  year={2019},
  pages={206-212}
}

@article{Wieser2012FacesIC,
  title={Faces in Context: A Review and Systematization of Contextual Influences on Affective Face Processing},
  author={Matthias J. Wieser and Tobias Brosch},
  journal={Frontiers in Psychology},
  year={2012},
  volume={3}
}

@ARTICLE{kostiPAMI,
  author={Kosti, Ronak and Alvarez, Jose M. and Recasens, Adria and Lapedriza, Agata},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Context Based Emotion Recognition Using EMOTIC Dataset}, 
  year={2020},
  volume={42},
  number={11},
  pages={2755-2766},
  doi={10.1109/TPAMI.2019.2916866}}


@INPROCEEDINGS{CAER-S,
  author={Lee, Jiyoung and Kim, Seungryong and Kim, Sunok and Park, Jungin and Sohn, Kwanghoon},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Context-Aware Emotion Recognition Networks}, 
  year={2019},
  volume={},
  number={},
  pages={10142-10151},
  doi={10.1109/ICCV.2019.01024}}

@INPROCEEDINGS{CAGER,
  author={Zhang, Minghui and Liang, Yumeng and Ma, Huadong},
  booktitle={2019 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Context-Aware Affective Graph Reasoning for Emotion Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={151-156},
  doi={10.1109/ICME.2019.00034}}

@inproceedings{ALBEF,
      title={Align before Fuse: Vision and Language Representation Learning with Momentum Distillation}, 
      author={Junnan Li and Ramprasaath R. Selvaraju and others},
      year={2021},
      booktitle={NeurIPS}
      }
@article{Dosovitskiy2021AnII,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and others},
  journal={ArXiv},
  year={2021},
  volume={abs/2010.11929}
}

 @article{zhou2017places,
   title={Places: A 10 million Image Database for Scene Recognition},
   author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
   journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
   year={2017},
   publisher={IEEE}
 }

@article{He2016DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@inproceedings{Devlin2019BERT,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL},
  year={2019}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and others",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}

@inProceedings{yu2019mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}

@inproceedings{AdamW,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={ICLR},
  year={2019}
}

@article{Adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@article{MTCNN,
  title={Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
  author={Kaipeng Zhang and Zhanpeng Zhang and Zhifeng Li and Yu Qiao},
  journal={IEEE Signal Processing Letters},
  year={2016},
  volume={23},
  pages={1499-1503}
}
@article{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and Sam Gross and Francisco Massa and others},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.01703}
}

@book{Pardun,
author = {Pardun, J Carol},
title = {Advertising and Society : an Introduction},
year = {2013},
isbn = {978-0-470-67309-6},
publisher = {John Wiley \& Sons, Inc.}, address = {New York, NY, USA},
}

  @article{Kim2017WhyNA,
  title={Why Narrative Ads Work: An Integrated Process Explanation},
  author={Eunjin Anna Kim and Srinivasan Ratneshwar and Esther Thorson},
  journal={Journal of Advertising},
  year={2017},
  volume={46},
  pages={283 - 296}
}

@article{Puto1984InformationalAT,
  title={Informational and Transformational Advertising: the Differential Effects of Time},
  author={Christopher P. Puto and William D. Wells},
  journal={ACR North American Advances},
  year={1984}
}

@article{Leong1994UsingDT,
  title={Using Drama to Persuade: the Effects of Involvement and Ad Form on Persuasion},
  author={Siew Meng Leong and Swee Hoon Ang and Lynn Heng},
  journal={ACR Asia-Pacific Advances},
  year={1994}
}

@inproceedings{Mick1987TowardAS,
  title={Toward a Semiotic of Advertising Story Grammars},
  author={David Mick},
  year={1987}
}

@article{Veer2008HowTT,
  title={How the tone and wording of advertisements interact},
  author={Ekant Veer and Simon Pervan},
  journal={International Journal of Advertising},
  year={2008},
  volume={27},
  pages={191 - 207}
}


@article{Brooks2020ExploringAO,
  title={Exploring Ads of the World: How Social Issues Are Framed in Global Advertisements},
  author={Mary Elizabeth Brooks and Clay M. Craig and Shannon L. Bichard},
  journal={Howard Journal of Communications},
  year={2020},
  volume={31},
  pages={150 - 170}
}

@article{Lien2013NarrativeAT,
  title={Narrative ads: The effect of argument strength and story format},
  author={Nai-Hwa Lien and Yi-Ling Chen},
  journal={Journal of Business Research},
  year={2013},
  volume={66},
  pages={516-522}
}

@inproceedings{Fisher1987HumanCA,
  title={Human Communication As Narration: Toward a Philosophy of Reason, Value and Action},
  author={Walter R. Fisher},
  year={1987}
}

@inproceedings{Escalas1998ADVERTISINGNW,
  title={ADVERTISING NARRATIVES: What are they and how do they work?},
  author={Jennifer Edson Escalas},
  year={1998}
}

@article{Lien2013NarrativeAT,
  title={Narrative ads: The effect of argument strength and story format},
  author={Nai-Hwa Lien and Yi-Ling Chen},
  journal={Journal of Business Research},
  year={2013},
  volume={66},
  pages={516-522}
}

@article{Micu2010MeasurableEH,
  title={Measurable Emotions: How Television Ads Really Work},
  author={Anca C. Micu and Joseph T. Plummer},
  journal={Journal of Advertising Research},
  year={2010},
  volume={50},
  pages={137 - 153}
}

@misc{google-diversity,
author = {Google},
title = {Diversity and inclusion in advertisement videos},
url = {https://www.thinkwithgoogle.com/feature/diversity-inclusion/?vertical=All}
}

@inproceedings{Fisher1987HumanCA,
  title={Human Communication As Narration: Toward a Philosophy of Reason, Value and Action},
  author={Walter R. Fisher},
  year={1987}
}


@inproceedings{Escalas1998ADVERTISINGNW,
  title={ADVERTISING NARRATIVES: What are they and how do they work?},
  author={Jennifer Edson Escalas},
  year={1998}
}

@article{Lien2013NarrativeAT,
  title={Narrative ads: The effect of argument strength and story format},
  author={Nai-Hwa Lien and Yi-Ling Chen},
  journal={Journal of Business Research},
  year={2013},
  volume={66},
  pages={516-522}
}

@article{Li2017AnnotatingHS,
  title={Annotating High-Level Structures of Short Stories and Personal Anecdotes},
  author={Boyang Albert Li and Beth Cardier and Tong Wang and Florian Metze},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.06917}
}

@inproceedings{Ouyang2015ModelingRE,
  title={Modeling Reportable Events as Turning Points in Narrative},
  author={Jessica Ouyang and Kathleen McKeown},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2015}
}

@article{Boyd2020TheNA,
  title={The narrative arc: Revealing core narrative structures through text analysis},
  author={Ryan L. Boyd and Kate G. Blackburn and James W. Pennebaker},
  journal={Science Advances},
  year={2020},
  volume={6}
}

@article{Holbrook1984TheRO,
  title={The role of emotion in advertising},
  author={Morris B. Holbrook and John J. O'Shaughnessy},
  journal={Psychology \& Marketing},
  year={1984},
  volume={1},
  pages={45-64}
}

@article{McDuff2014PredictingAL,
  title={Predicting Ad Liking and Purchase Intent: Large-Scale Analysis of Facial Responses to Ads},
  author={Daniel J. McDuff and Rana El Kaliouby and Jeffrey F. Cohn and Rosalind W. Picard},
  journal={IEEE Transactions on Affective Computing},
  year={2014},
  volume={6},
  pages={223-235}
}

@article{Teixeira2014WhyWA,
  title={Why, When, and How Much to Entertain Consumers in Advertisements? A Web-Based Facial Tracking Field Study},
  author={Thales S. Teixeira and Rosalind W. Picard and Rana El Kaliouby},
  journal={Mark. Sci.},
  year={2014},
  volume={33},
  pages={809-827}
}

@article{Shukla2017AffectRI,
  title={Affect Recognition in Ads with Application to Computational Advertising},
  author={Abhinav Shukla and Shruti Shriya Gullapuram and Harish Katti and Karthik Yadati and M. Kankanhalli and Subramanian Ramanathan},
  journal={Proceedings of the 25th ACM international conference on Multimedia},
  year={2017}
}

@article{Shukla2017EvaluatingCV,
  title={Evaluating content-centric vs. user-centric ad affect recognition},
  author={Abhinav Shukla and Shruti Shriya Gullapuram and Harish Katti and Karthik Yadati and M. Kankanhalli and Subramanian Ramanathan},
  journal={Proceedings of the 19th ACM International Conference on Multimodal Interaction},
  year={2017}
}

@article{Shukla2019RecognitionOA,
  title={Recognition of Advertisement Emotions With Application to Computational Advertising},
  author={Abhinav Shukla and Shruti Shriya Gullapuram and Harish Katti and M. Kankanhalli and Stefan Winkler and Subramanian Ramanathan},
  journal={IEEE Transactions on Affective Computing},
  year={2019},
  volume={13},
  pages={781-792}
}


@inproceedings{DouglasCowie2007TheHD,
  title={The HUMAINE Database: Addressing the Collection and Annotation of Naturalistic and Induced Emotional Data},
  author={Ellen Douglas-Cowie and Roddy Cowie and Ian Sneddon and Cate Cox and Orla Lowry and Margaret McRorie and Jean-Claude Martin and Laurence Devillers and Sarkis Abrilian and Anton Batliner and Noam Amir and Kostas Karpouzis},
  booktitle={Affective Computing and Intelligent Interaction},
  year={2007}
}

@article{Zlatintsi2017COGNIMUSEAM,
  title={COGNIMUSE: a multimodal video database annotated with saliency, events, semantics and emotion with application to summarization},
  author={Athanasia Zlatintsi and Petros Koutras and Georgios Evangelopoulos and Nikos Malandrakis and Niki Efthymiou and Katerina Pastra and Alexandros Potamianos and Petros Maragos},
  journal={EURASIP Journal on Image and Video Processing},
  year={2017},
  volume={2017},
  pages={1-24}
}

@article{Baveye2015LIRISACCEDEAV,
  title={LIRIS-ACCEDE: A Video Database for Affective Content Analysis},
  author={Yoann Baveye and Emmanuel Dellandr{\'e}a and Christel Chamaret and Liming Luke Chen},
  journal={IEEE Transactions on Affective Computing},
  year={2015},
  volume={6},
  pages={43-55}
}

@article{Sun2020EEVDP,
  title={EEV Dataset: Predicting Expressions Evoked by Diverse Videos},
  author={Jennifer J. Sun and Ting Liu and Alan S. Cowen and Florian Schroff and Hartwig Adam and Gautam Prasad},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.05488}
}

@article{Koelstra2012DEAPAD,
  title={DEAP: A Database for Emotion Analysis ;Using Physiological Signals},
  author={Sander Koelstra and Christian M{\"u}hl and M. Soleymani and Jong-Seok Lee and Ashkan Yazdani and Touradj Ebrahimi and Thierry Pun and Anton Nijholt and I. Patras},
  journal={IEEE Transactions on Affective Computing},
  year={2012},
  volume={3},
  pages={18-31}
}

@inproceedings{Jiang2014PredictingEI,
  title={Predicting Emotions in User-Generated Videos},
  author={Yu-Gang Jiang and Baohan Xu and X. Xue},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2014}
}

@inproceedings{Jiang2014PredictingEI,
  title={Predicting Emotions in User-Generated Videos},
  author={Yu-Gang Jiang and Baohan Xu and X. Xue},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2014}
}

@article{Ong2019ModelingEI,
  title={Modeling Emotion in Complex Stories: The Stanford Emotional Narratives Dataset},
  author={Desmond C. Ong and Zhengxuan Wu and Zhi-Xuan Tan and Marianne C. Reddan and Isabella Kahhal{\'e} and Alison Mattek and Jamil Zaki},
  journal={IEEE Transactions on Affective Computing},
  year={2019},
  volume={12},
  pages={579-594}
}

@article{Hussain2017AutomaticUO,
  title={Automatic Understanding of Image and Video Advertisements},
  author={Zaeem Hussain and Mingda Zhang and Xiaozhong Zhang and Keren Ye and Christopher Thomas and Zuha Agha and Nathan Ong and Adriana Kovashka},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={1100-1110}
}


@inproceedings{Ye2018StoryUI,
  title={Story Understanding in Video Advertisements},
  author={Keren Ye and Kyle Buettner and Adriana Kovashka},
  booktitle={British Machine Vision Conference},
  year={2018}
}


@inproceedings{bain2020condensed,
  title={Condensed movies: Story based retrieval with contextual embeddings},
  author={Bain, Max and Nagrani, Arsha and Brown, Andrew and Zisserman, Andrew},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  year={2020}
}

@inproceedings{huang2020movienet,
  title={Movienet: A holistic dataset for movie understanding},
  author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part IV 16},
  pages={709--727},
  year={2020},
  organization={Springer}
}

@InProceedings{Bose_2023_WACV,
    author    = {Bose, Digbalay and Hebbar, Rajat and Somandepalli, Krishna and Zhang, Haoyang and Cui, Yin and Cole-McLaughlin, Kree and Wang, Huisheng and Narayanan, Shrikanth},
    title     = {MovieCLIP: Visual Scene Recognition in Movies},
    booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    month     = {January},
    year      = {2023},
    pages     = {2083-2092}
}

@InProceedings{Soldan_2022_CVPR,
    author    = {Soldan, Mattia and Pardo, Alejandro and Alc\'azar, Juan Le\'on and Caba, Fabian and Zhao, Chen and Giancola, Silvio and Ghanem, Bernard},
    title     = {MAD: A Scalable Dataset for Language Grounding in Videos From Movie Audio Descriptions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {5026-5035}
}

@inproceedings{Pardo2021MovieCutsAN,
  title={MovieCuts: A New Dataset and Benchmark for Cut Type Recognition},
  author={A. Pardo and Fabian Caba Heilbron and Juan Le'on Alc'azar and Ali K. Thabet and Bernard Ghanem},
  booktitle={European Conference on Computer Vision},
  year={2021}
}

@article{Hebbar2023ADF,
  title={A dataset for Audio-Visual Sound Event Detection in Movies},
  author={Rajat Hebbar and Digbalay Bose and Krishna Somandepalli and Veena Vijai and Shrikanth S. Narayanan},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.07315}
}

@article{Singla2022PersuasionSI,
  title={Persuasion Strategies in Advertisements: Dataset, Modeling, and Baselines},
  author={Yaman Kumar Singla and Rajat Aayush Jha and Arunim Gupta and Milan Aggarwal and Aditya Garg and Ayushi Bhardwaj and Tushar and Balaji Krishnamurthy and Rajiv Ratn Shah and Changyou Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.09626}
}

@article{Jiang2022TencentAA,
  title={Tencent AVS: A Holistic Ads Video Dataset for Multi-Modal Scene Segmentation},
  author={Jie Jiang and Zhimin Li and Jiangfeng Xiong and Rongwei Quan and Qinglin Lu and Wei Liu},
  journal={IEEE Access},
  year={2022},
  volume={10},
  pages={128959-128969}
}

@article{Zhang2022AttractMT,
  title={Attract me to Buy: Advertisement Copywriting Generation with Multimodal Multi-structured Information},
  author={Zhipeng Zhang and Xinglin Hou and Kai Niu and Zhongzhen Huang and Tiezheng Ge and Yuning Jiang and Qi Wu and Peifeng Wang},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.03534}
}

@article{Carreira2017QuoVA,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733}
}

@article{Monfort2018MomentsIT,
  title={Moments in Time Dataset: One Million Videos for Event Understanding},
  author={Mathew Monfort and Bolei Zhou and Sarah Adel Bargal and Alex Andonian and Tom Yan and Kandan Ramakrishnan and Lisa M. Brown and Quanfu Fan and Dan Gutfreund and Carl Vondrick and Aude Oliva},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2018},
  volume={42},
  pages={502-508}
}

@article{Heilbron2015ActivityNetAL,
  title={ActivityNet: A large-scale video benchmark for human activity understanding},
  author={Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={961-970}
}

@article{Gu2017AVAAV,
  title={AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},
  author={Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and Caroline Pantofaru and David A. Ross and George Toderici and Yeqing Li and Susanna Ricco and Rahul Sukthankar and Cordelia Schmid and Jitendra Malik},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2017},
  pages={6047-6056}
}

@article{AbuElHaija2016YouTube8MAL,
  title={YouTube-8M: A Large-Scale Video Classification Benchmark},
  author={Sami Abu-El-Haija and Nisarg Kothari and Joonseok Lee and Apostol Natsev and George Toderici and Balakrishnan Varadarajan and Sudheendra Vijayanarasimhan},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.08675}
}

@article{Gupta20223MASSIVMM,
  title={3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos},
  author={Vikram Gupta and Trisha Mittal and Puneet Mathur and Vaibhav Mishra and Mayank Maheshwari and Aniket Bera and Debdoot Mukherjee and Dinesh Manocha},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={21032-21043}
}

@inproceedings{diba2020large,
  title={Large scale holistic video understanding},
  author={Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, J{\"u}rgen and Stiefelhagen, Rainer and Van Gool, Luc},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part V 16},
  pages={593--610},
  year={2020},
  organization={Springer}
}

@article{Baltruaitis2017MultimodalML,
  title={Multimodal Machine Learning: A Survey and Taxonomy},
  author={Tadas Baltru{\vs}aitis and Chaitanya Ahuja and Louis-Philippe Morency},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  volume={41},
  pages={423-443}
}

@article{Liang2022FoundationsAR,
  title={Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  author={Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.03430}
}

@inproceedings{Akbari2021VATTTF,
  title={VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text},
  author={Hassan Akbari and Linagzhe Yuan and Rui Qian and Wei-Hong Chuang and Shih-Fu Chang and Yin Cui and Boqing Gong},
  booktitle={Neural Information Processing Systems},
  year={2021}
}

@article{Jaegle2021PerceiverIA,
  title={Perceiver IO: A General Architecture for Structured Inputs \& Outputs},
  author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Andrew Brock and Evan Shelhamer and Olivier J. H'enaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Jo{\~a}o Carreira},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.14795}
}

@article{nagrani2021attention,
  title={Attention bottlenecks for multimodal fusion},
  author={Nagrani, Arsha and Yang, Shan and Arnab, Anurag and Jansen, Aren and Schmid, Cordelia and Sun, Chen},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14200--14213},
  year={2021}
}

@inproceedings{SomandepJointenc,
author = {Somandepalli, Krishna and Martinez, Victor and Kumar, Naveen and Narayanan, Shrikanth},
title = {Multimodal Representation of Advertisements Using Segment-Level Autoencoders},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.libproxy1.usc.edu/10.1145/3242969.3243026},
doi = {10.1145/3242969.3243026},
pages = {418–422},
numpages = {5},
keywords = {autoencoders, advertisements, multimodal joint representation},
location = {Boulder, CO, USA},
series = {ICMI '18}
}}

@article{Ye2019InterpretingTR,
  title={Interpreting the Rhetoric of Visual Advertisements},
  author={Keren Ye and Narges Honarvar Nazari and James Hahn and Zaeem Hussain and Mingda Zhang and Adriana Kovashka},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2019},
  volume={43},
  pages={1308-1323}
}

@inproceedings{LookReadFeel,
author = {Zhang, Huaizheng and Luo, Yong and Ai, Qiming and Wen, Yonggang and Hu, Han},
title = {Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.libproxy2.usc.edu/10.1145/3394171.3413582},
doi = {10.1145/3394171.3413582},
abstract = {Given the massive market of advertising and the sharply increasing online multimedia content (such as videos), it is now fashionable to promote advertisements (ads) together with the multimedia content. However, manually finding relevant ads to match the provided content is labor-intensive, and hence some automatic advertising techniques are developed. Since ads are usually hard to understand only according to its visual appearance due to the contained visual metaphor, some other modalities, such as the contained texts, should be exploited for understanding. To further improve user experience, it is necessary to understand both the ads' topic and sentiment. This motivates us to develop a novel deep multimodal multitask framework that integrates multiple modalities to achieve effective topic and sentiment prediction simultaneously for ads understanding. In particular, in our framework termed Deep$M^2$Ad, we first extract multimodal information from ads and learn high-level and comparable representations. The visual metaphor of the ad is decoded in an unsupervised manner. The obtained representations are then fed into the proposed hierarchical multimodal attention modules to learn task-specific representations for final prediction. A multitask loss function is also designed to jointly train both the topic and sentiment prediction models in an end-to-end manner, where bottom-layer parameters are shared to alleviate over-fitting. We conduct extensive experiments on a large-scale advertisement dataset and achieve state-of-the-art performance for both prediction tasks. The obtained results could be utilized as a benchmark for ads understanding.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {430–438},
numpages = {9},
keywords = {multimodal learning, online advertising, ads understanding, multitask learning, neural networks},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{Lin2018NeXtVLADAE,
  title={NeXtVLAD: An Efficient Neural Network to Aggregate Frame-level Features for Large-scale Video Classification},
  author={Rongcheng Lin and Jing Xiao and Jianping Fan},
  booktitle={ECCV Workshops},
  year={2018}
}

@article{Weng2021AMF,
  title={A Multimodal Framework for Video Ads Understanding},
  author={Zejia Weng and Lingjiang Meng and Rui Wang and Zuxuan Wu and Yu-Gang Jiang},
  journal={Proceedings of the 29th ACM International Conference on Multimedia},
  year={2021}
}

@article{Vedula2017MultimodalCA,
  title={Multimodal Content Analysis for Effective Advertisements on YouTube},
  author={Nikhita Vedula and Wei Sun and Hyunhwan Lee and Harsh Gupta and Mitsunori Ogihara and Joseph Johnson and Gang Ren and Srinivasan Parthasarathy},
  journal={2017 IEEE International Conference on Data Mining (ICDM)},
  year={2017},
  pages={1123-1128}
}

@misc{cannes-lions,
author = {Cannes},
title = {Cannes Lions },
url = {https://www.canneslions.com/}
}

@book{ciment2006social,
  author = {Ciment, James},
  year = {2006},
  title = {Social Issues in America: An Encyclopedia},
  publisher = {M.E. Sharpe},
  address = {Armonk, NY},
}

@article{Radford2022RobustSR,
  title={Robust Speech Recognition via Large-Scale Weak Supervision},
  author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.04356}
}

@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}


@inproceedings{gong21b_interspeech,
  author={Yuan Gong and Yu-An Chung and James Glass},
  title={{AST: Audio Spectrogram Transformer}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={571--575},
  doi={10.21437/Interspeech.2021-698}
}

@article{Gemmeke2017AudioSA,
  title={Audio Set: An ontology and human-labeled dataset for audio events},
  author={Jort F. Gemmeke and Daniel P. W. Ellis and Dylan Freedman and Aren Jansen and Wade Lawrence and R. Channing Moore and Manoj Plakal and Marvin Ritter},
  journal={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2017},
  pages={776-780}
}

@article{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{Iyer2022OPTIMLSL,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Srinivas Iyer and Xiaojuan Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2022},
  volume={abs/2212.12017}
}

@article{Chung2022ScalingIL,
  title={Scaling Instruction-Finetuned Language Models},
  author={Hyung Won Chung and Le Hou and S. Longpre and others},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.11416}
}

@article{Bommasani2021OnTO,
  title={On the Opportunities and Risks of Foundation Models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and et.al},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.07258}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and others},
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{Zhao2023ASO,
  title={A Survey of Large Language Models},
  author={Wayne Xin Zhao and Kun Zhou and Junyi Li and others},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.18223}
}

@article{Jaegle2021PerceiverIA,
  title={Perceiver IO: A General Architecture for Structured Inputs \& Outputs},
  author={Andrew Jaegle and Sebastian Borgeaud and Jean-Baptiste Alayrac and Carl Doersch and Catalin Ionescu and David Ding and Skanda Koppula and Andrew Brock and Evan Shelhamer and Olivier J. H'enaff and Matthew M. Botvinick and Andrew Zisserman and Oriol Vinyals and Jo{\~a}o Carreira},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.14795}
}

@article{Wei2022ChainOT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Huai-hsin Chi and F. Xia and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903}
}

@article{wang2022MMPTMSurvey,
  title={Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey},
  author={Wang, Xiao and Chen, Guangyao and Qian, Guangwu and Gao, Pengcheng and Wei, Xiao-Yong and Wang, Yaowei and Tian, Yonghong and Gao, Wen},
  url={https://github.com/wangxiao5791509/MultiModal_BigModels_Survey},
  year={2022}
}

@InProceedings{Bain21,
  author       = "Max Bain and Arsha Nagrani and G{\"u}l Varol and Andrew Zisserman",
  title        = "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval",
  booktitle    = "IEEE International Conference on Computer Vision",
  year         = "2021",
}

@inproceedings{
schuhmann2022laionb,
title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},
author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
url={https://openreview.net/forum?id=M3Y74vmsMcY}
}


@inproceedings{changpinyo2021cc12m,
  title = {{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle = {CVPR},
  year = {2021},
}

@article{Tishby2015DeepLA,
  title={Deep learning and the information bottleneck principle},
  author={Naftali Tishby and Noga Zaslavsky},
  journal={2015 IEEE Information Theory Workshop (ITW)},
  year={2015},
  pages={1-5},
  url={https://api.semanticscholar.org/CorpusID:5541663}
}

@article{Kiela2020TheHM,
  title={The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes},
  author={Douwe Kiela and Hamed Firooz and Aravind Mohan and Vedanuj Goswami and Amanpreet Singh and Pratik Ringshia and Davide Testuggine},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.04790},
  url={https://api.semanticscholar.org/CorpusID:218581273}
}

@article{Arevalo2017GatedMU,
  title={Gated Multimodal Units for Information Fusion},
  author={John Arevalo and Thamar Solorio and Manuel Montes-y-G{\'o}mez and Fabio A. Gonz{\'a}lez},
  journal={ArXiv},
  year={2017},
  volume={abs/1702.01992},
  url={https://api.semanticscholar.org/CorpusID:9401721}
}

@article{Li2023BLIP2BL,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.12597},
  url={https://api.semanticscholar.org/CorpusID:256390509}
}

@inproceedings{Kim2021ViLTVT,
  title={ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  author={Wonjae Kim and Bokyung Son and Ildoo Kim},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231839613}
}

@ARTICLE{uavm_gong,
  author={Gong, Yuan and Liu, Alexander H. and Rouditchenko, Andrew and Glass, James},
  journal={IEEE Signal Processing Letters}, 
  title={UAVM: Towards Unifying Audio and Visual Models}, 
  year={2022},
  volume={29},
  pages={2437-2441},
  doi={10.1109/LSP.2022.3224688}}

@article{Mahabadi2021VariationalIB,
  title={Variational Information Bottleneck for Effective Low-Resource Fine-Tuning},
  author={Rabeeh Karimi Mahabadi and Yonatan Belinkov and James Henderson},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.05469},
  url={https://api.semanticscholar.org/CorpusID:235391000}
}

@inproceedings{
wang2021infobert,
title={InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Wang, Boxin and Wang, Shuohang and Cheng, Yu and Gan, Zhe and Jia, Ruoxi and Li, Bo and Liu, Jingjing},
booktitle={International Conference on Learning Representations},
year={2021}}

@article{Jiang2022CorrelationIB,
  title={Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering},
  author={Jingjing Jiang and Zi-yi Liu and Nanning Zheng},
  journal={International Journal of Computer Vision},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:257255497}
}