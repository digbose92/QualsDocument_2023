%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for ZHANG Ruda at 2016-04-04 14:14:11 -0700


%% Saved with string encoding Unicode (UTF-8)


@article{einstein_ist_1905,
	title = {Ist die Trägheit eines Körpers von seinem Energieinhalt abhängig?},
	volume = {323},
	rights = {Copyright © 1905 {WILEY}‐{VCH} Verlag {GmbH} \& Co. {KGaA}, Weinheim},
	issn = {1521-3889},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/andp.19053231314},
	doi = {10.1002/andp.19053231314},
	pages = {639--641},
	number = {13},
	journaltitle = {Annalen der Physik},
	author = {Einstein, A.},
	date = {1905}
}

@book{Fisher:1954,
	Address = {New York},
	Author = {Ronald Aylmer Fisher},
	Edition = {12th},
	Publisher = {Hafner},
	Title = {Statistical methods for research workers},
	Year = {1954}}

@article{Robbins:1951aa,
	Author = {Robbins, Herbert and Sutton Monro},
	Journal = {The Annals of Mathematical Statistics},
	Number = {3},
	Pages = {pp. 400-407},
	Title = {A Stochastic Approximation Method},
	Volume = {22},
	Year = {1951}}

@book{Knight:1921,
	Address = {Boston, New York},
	Author = {Frank H Knight},
	Publisher = {Houghton Mifflin Company},
	Title = {Risk, uncertainty and profit},
	Year = {1921}}

@article{Wright:1921aa,
	Author = {Wright, Sewall},
	Journal = {Journal of agricultural research},
	Pages = {557-585},
	Title = {Correlation and Causation},
	Year = {1921}}

@article{Caratheodory:1909aa,
	Author = {Constantin Carath{\'e}odory},
	Journal = {Mathematische Annalen},
	Pages = {355--386},
	Title = {Untersuchungen {\"u}ber die Grundlagen der Thermodynamik},
	Volume = {67},
	Year = {1909}}

@book{Gibbs:1902,
	Address = {New York},
	Author = {Gibbs, J Willard},
	Publisher = {C. Scribner's sons},
	Title = {Elementary principles in statistical mechanics: Developed with especial reference to the rational foundations of thermodynamics},
	Year = {1902}}

@article{Clausius:1857,
	Author = {Clausius, Rudolf},
	Journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	Number = {91},
	Pages = {108--127},
	Title = {XI. On the nature of the motion which we call heat},
	Volume = {14},
	Year = {1857}}

@article{contextvision,
author = {Wang, Xuan and Zhu, Zhigang},
title = {Context Understanding in Computer Vision: A Survey},
year = {2023},
issue_date = {Mar 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {229},
number = {C},
issn = {1077-3142},
url = {https://doi-org.libproxy2.usc.edu/10.1016/j.cviu.2023.103646},
doi = {10.1016/j.cviu.2023.103646},
journal = {Comput. Vis. Image Underst.},
month = {apr},
numpages = {28},
keywords = {65D17, Deep Learning Models, Context, 41A10, 41A05, Computer Vision, Context Integration, 65D05}
}

@article{Rabinovich2007ObjectsIC,
  title={Objects in Context},
  author={Andrew Rabinovich and Andrea Vedaldi and Carolina Galleguillos and Eric Wiewiora and Serge J. Belongie},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:749550}
}

@article{Wang2007ShapeAA,
  title={Shape and Appearance Context Modeling},
  author={Xiaogang Wang and Gianfranco Doretto and Thomas B. Sebastian and J. Rittscher and Peter H. Tu},
  journal={2007 IEEE 11th International Conference on Computer Vision},
  year={2007},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:14958192}
}

@ARTICLE {Faceness-Net,
author = {S. Yang and P. Luo and C. Loy and X. Tang},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Faceness-Net: Face Detection through Deep Facial Part Responses},
year = {2018},
volume = {40},
number = {08},
issn = {1939-3539},
pages = {1845-1859},
abstract = {We propose a deep convolutional neural network (CNN) for face detection leveraging on facial attributes based supervision. We observe a phenomenon that part detectors emerge within CNN trained to classify attributes from uncropped face images, without any explicit part supervision. The observation motivates a new method for finding faces through scoring facial parts responses by their spatial structure and arrangement. The scoring mechanism is data-driven, and carefully formulated considering challenging cases where faces are only partially visible. This consideration allows our network to detect faces under severe occlusion and unconstrained pose variations. Our method achieves promising performance on popular benchmarks including FDDB, PASCAL Faces, AFW, and WIDER FACE.},
keywords = {face;proposals;face detection;detectors;neural networks;mouth;training},
doi = {10.1109/TPAMI.2017.2738644},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@article{Marques2010ContextMI,
  title={Context modeling in computer vision: techniques, implications, and applications},
  author={Oge Marques and Elan Barenholtz and Vincent Charvillat},
  journal={Multimedia Tools and Applications},
  year={2010},
  volume={51},
  pages={303-339},
  url={https://api.semanticscholar.org/CorpusID:18206222}
}

@article{Chang2021ACS,
  title={A Comprehensive Survey of Scene Graphs: Generation and Application},
  author={Xiaojun Chang and Pengzhen Ren and Pengfei Xu and Zhihui Li and Xiaojiang Chen and Alexander G. Hauptmann},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={45},
  pages={1-26},
  url={https://api.semanticscholar.org/CorpusID:245445853}
}

@article{Xu2017SceneGG,
  title={Scene Graph Generation by Iterative Message Passing},
  author={Danfei Xu and Yuke Zhu and Christopher Bongsoo Choy and Li Fei-Fei},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={3097-3106},
  url={https://api.semanticscholar.org/CorpusID:1780254}
}

@article{Yan2019LearningCG,
  title={Learning Context Graph for Person Search},
  author={Yichao Yan and Qiang Zhang and Bingbing Ni and Wendong Zhang and Minghao Xu and Xiaokang Yang},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={2153-2162},
  url={https://api.semanticscholar.org/CorpusID:102487416}
}

@article{Carreira2017QuoVA,
  title={Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  author={Jo{\~a}o Carreira and Andrew Zisserman},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={4724-4733},
  url={https://api.semanticscholar.org/CorpusID:206596127}
}

@article{Ji2021DetectingHR,
  title={Detecting Human-Object Relationships in Videos},
  author={Jingwei Ji and Rishi Desai and Juan Carlos Niebles},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={8086-8096},
  url={https://api.semanticscholar.org/CorpusID:242213105}
}

@article{Wu2021TowardsLV,
  title={Towards Long-Form Video Understanding},
  author={Chaoxia Wu and Philipp Kr{\"a}henb{\"u}hl},
  journal={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={1884-1894},
  url={https://api.semanticscholar.org/CorpusID:235416690}
}

@article{Soldan2021MADAS,
  title={MAD: A Scalable Dataset for Language Grounding in Videos from Movie Audio Descriptions},
  author={Mattia Soldan and A. Pardo and Juan Le'on Alc'azar and Fabian Caba Heilbron and Chen Zhao and Silvio Giancola and Bernard Ghanem},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021},
  pages={5016-5025},
  url={https://api.semanticscholar.org/CorpusID:244773187}
}

@article{Beery2019ContextRL,
  title={Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection},
  author={Sara Beery and Guanhang Wu and Vivek Rathod and Ronny Votel and Jonathan Huang},
  journal={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={13072-13082},
  url={https://api.semanticscholar.org/CorpusID:208921095}
}

@article{purushwalkam2020audio,
  title={Audio-visual floorplan reconstruction},
  author={Purushwalkam, Senthil and Gari, Sebastian Vicenc Amengual and Ithapu, Vamsi Krishna and Schissler, Carl and Robinson, Philip and Gupta, Abhinav and Grauman, Kristen},
  journal={arXiv preprint arXiv:2012.15470},
  year={2020}
}

@inproceedings{chen2020soundspaces,
    title = {SoundSpaces: Audio-Visual Navigation in 3D Environments},
    author = {Chen, Changan and Jain, Unnat and Schissler, Carl and Gari, Sebastia Vicenc Amengual and Al-Halah, Ziad and Ithapu, Vamsi Krishna and Robinson, Philip and Grauman, Kristen},
    year = {2020},
    booktitle={ECCV},
}

@article{Tian2018AudioVisualEL,
  title={Audio-Visual Event Localization in Unconstrained Videos},
  author={Yapeng Tian and Jing Shi and Bochen Li and Zhiyao Duan and Chenliang Xu},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.08842},
  url={https://api.semanticscholar.org/CorpusID:4336836}
}

@article{Zhang2022TemporalSG,
  title={Temporal Sentence Grounding in Videos: A Survey and Future Directions},
  author={Hao Zhang and Aixin Sun and Wei Jing and Joey Tianyi Zhou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  volume={45},
  pages={10443-10465},
  url={https://api.semanticscholar.org/CorpusID:253224445}
}

@article{Seymour2017AutomatedDA,
  title={Automated detection and enumeration of marine wildlife using unmanned aircraft systems (UAS) and thermal imagery},
  author={Alexander C. Seymour and Julian Dale and Mike O. Hammill and Patrick N. Halpin and David W. Johnston},
  journal={Scientific Reports},
  year={2017},
  volume={7},
  url={https://api.semanticscholar.org/CorpusID:14140341}
}

@article{Baltruaitis2017MultimodalML,
  title={Multimodal Machine Learning: A Survey and Taxonomy},
  author={Tadas Baltru{\vs}aitis and Chaitanya Ahuja and Louis-Philippe Morency},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  volume={41},
  pages={423-443},
  url={https://api.semanticscholar.org/CorpusID:10137425}
}

@article{Liang2022FoundationsAR,
  title={Foundations and Recent Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  author={Paul Pu Liang and Amir Zadeh and Louis-Philippe Morency},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.03430},
  url={https://api.semanticscholar.org/CorpusID:252118396}
}

@article{Gao2020ASO,
  title={A Survey on Deep Learning for Multimodal Data Fusion},
  author={Jing Gao and Peng Li and Zhikui Chen and Jianing Zhang},
  journal={Neural Computation},
  year={2020},
  volume={32},
  pages={829-864},
  url={https://api.semanticscholar.org/CorpusID:212748233}
}

@article{Abdar2023ARO,
  title={A Review of Deep Learning for Video Captioning},
  author={Moloud Abdar and Meenakshi Kollati and Swaraja Kuraparthi and Farhad Pourpanah and Daniel J. McDuff and Mohammad Ghavamzadeh and Shuicheng Yan and Abduallah A. Mohamed and Abbas Khosravi and E. Cambria and Fatih Murat Porikli},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11431},
  url={https://api.semanticscholar.org/CorpusID:258298547}
}

@article{Apostolidis2021VideoSU,
  title={Video Summarization Using Deep Neural Networks: A Survey},
  author={Evlampios Apostolidis and E. Adamantidou and Alexandros I. Metsai and Vasileios Mezaris and I. Patras},
  journal={Proceedings of the IEEE},
  year={2021},
  volume={109},
  pages={1838-1863},
  url={https://api.semanticscholar.org/CorpusID:231627658}
}

@misc{defaria2023visual,
      title={Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature}, 
      author={Ana Cláudia Akemi Matsuki de Faria and Felype de Castro Bastos and José Victor Nogueira Alves da Silva and Vitor Lopes Fabris and Valeska de Sousa Uchoa and Décio Gonçalves de Aguiar Neto and Claudio Filipi Goncalves dos Santos},
      year={2023},
      eprint={2305.11033},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{Alain2001WhatA,
  title={“What” and “where” in the human auditory system},
  author={Claude Alain and Stephen R. Arnott and Stephanie J. Hevenor and Stephen Graham and Cheryl L. Grady},
  journal={Proceedings of the National Academy of Sciences of the United States of America},
  year={2001},
  volume={98},
  pages={12301 - 12306},
  url={https://api.semanticscholar.org/CorpusID:25136641}
}

@article{BornkesselSchlesewsky2015NeurobiologicalRO,
  title={Neurobiological roots of language in primate audition: common computational properties},
  author={Ina Bornkessel-Schlesewsky and Matthias Schlesewsky and Steven L. Small and Josef P. Rauschecker},
  journal={Trends in Cognitive Sciences},
  year={2015},
  volume={19},
  pages={142-150},
  url={https://api.semanticscholar.org/CorpusID:11071265}
}

@article{Wallace2002HistochemicalIO,
  title={Histochemical identification of cortical areas in the auditory region of the human brain},
  author={Mark N. Wallace and Peter W. Johnston and Alan R. Palmer},
  journal={Experimental Brain Research},
  year={2002},
  volume={143},
  pages={499-508},
  url={https://api.semanticscholar.org/CorpusID:24211906}
}

@article{Torralba2003ContextualPF,
  title={Contextual Priming for Object Detection},
  author={Antonio Torralba},
  journal={International Journal of Computer Vision},
  year={2003},
  volume={53},
  pages={169-191},
  url={https://api.semanticscholar.org/CorpusID:1073705}
}

@article{Choi2012ContextMA,
  title={Context models and out-of-context objects},
  author={Myung Jin Choi and Antonio Torralba and Alan S. Willsky},
  journal={Pattern Recognit. Lett.},
  year={2012},
  volume={33},
  pages={853-862},
  url={https://api.semanticscholar.org/CorpusID:8354810}
}

@article{barrettcontext,
author = {Lisa Feldman Barrett and Batja Mesquita and Maria Gendron},
title ={Context in Emotion Perception},
journal = {Current Directions in Psychological Science},
volume = {20},
number = {5},
pages = {286-290},
year = {2011},
doi = {10.1177/0963721411422522},

URL = { 
        https://doi.org/10.1177/0963721411422522
    
},
eprint = { 
        https://doi.org/10.1177/0963721411422522
    
}
,
    abstract = { We review recent work demonstrating consistent context effects during emotion perception. Visual scenes, voices, bodies, other faces, cultural orientation, and even words shape how emotion is perceived in a face, calling into question the still-common assumption that the emotional state of a person is written on and can be read from the face like words on a page. Incorporating context during emotion perception appears to be routine, efficient, and, to some degree, automatic. This evidence challenges the standard view of emotion perception represented in psychology texts, in the cognitive neuroscience literature, and in the popular media and points to a necessary change in the basic paradigm used in the scientific study of emotion perception. }
}

@inproceedings{Zhao2022M3EDMM,
  title={M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database},
  author={Jinming Zhao and Tenggan Zhang and Jingwen Hu and Yuchen Liu and Qin Jin and Xinchao Wang and Haizhou Li},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:248780564}
}

@article{Das2016VisualD,
  title={Visual Dialog},
  author={Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos{\'e} M. F. Moura and Devi Parikh and Dhruv Batra},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={1080-1089},
  url={https://api.semanticscholar.org/CorpusID:1820614}
}

@ARTICLE{CMI,
  author={Somandepalli, Krishna and Guha, Tanaya and Martinez, Victor R. and Kumar, Naveen and Adam, Hartwig and Narayanan, Shrikanth},
  journal={Proceedings of the IEEE}, 
  title={Computational Media Intelligence: Human-Centered Machine Analysis of Media}, 
  year={2021},
  volume={109},
  number={5},
  pages={891-910},
  doi={10.1109/JPROC.2020.3047978}}

@inproceedings{Radford2021LearningTV,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:231591445}
}

@article{Bar2004VisualOI,
  title={Visual objects in context},
  author={Moshe Bar},
  journal={Nature Reviews Neuroscience},
  year={2004},
  volume={5},
  pages={617-629},
  url={https://api.semanticscholar.org/CorpusID:205499985}
}

@article{Qiao2021ObjectLevelSC,
  title={Object-Level Scene Context Prediction},
  author={Xiaotian Qiao and Quanlong Zheng and Ying Cao and Rynson W. H. Lau},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  volume={44},
  pages={5280-5292},
  url={https://api.semanticscholar.org/CorpusID:233428370}
}


@ARTICLE{CMI,
  author={Somandepalli, Krishna and Guha, Tanaya and Martinez, Victor R. and Kumar, Naveen and Adam, Hartwig and Narayanan, Shrikanth},
  journal={Proceedings of the IEEE}, 
  title={Computational Media Intelligence: Human-Centered Machine Analysis of Media}, 
  year={2021},
  volume={109},
  number={5},
  pages={891-910},
  doi={10.1109/JPROC.2020.3047978}}

@inproceedings{Bordwell1979FilmAA,
  title={Film Art: An Introduction},
  author={David Bordwell and Kristin Thompson},
  year={1979},
  url={https://api.semanticscholar.org/CorpusID:190106329}
}

@article{Xiao2010SUNDL,
  title={SUN database: Large-scale scene recognition from abbey to zoo},
  author={Jianxiong Xiao and James Hays and Krista A. Ehinger and Aude Oliva and Antonio Torralba},
  journal={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year={2010},
  pages={3485-3492},
  url={https://api.semanticscholar.org/CorpusID:1309931}
}

@article{zhou2017places,
  title={Places: A 10 million Image Database for Scene Recognition},
  author={Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2017},
  publisher={IEEE}
}

@misc{bain2020condensed,
    title={Condensed Movies: Story Based Retrieval with Contextual Embeddings},
    author={Max Bain and Arsha Nagrani and Andrew Brown and Andrew Zisserman},
    year={2020},
    eprint={2005.04208},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@InProceedings{Sadhu_2021_CVPR,
          author = {Sadhu, Arka and Gupta, Tanmay and Yatskar, Mark and Nevatia, Ram and Kembhavi, Aniruddha},
          title = {Visual Semantic Role Labeling for Video Understanding},
          booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
          month = {June},
          year = {2021}}


@InProceedings{SBD,
author="Helm, Daniel
and Kampel, Martin",
editor="Cristani, Marco
and Prati, Andrea
and Lanz, Oswald
and Messelodi, Stefano
and Sebe, Nicu",
title="Shot Boundary Detection for Automatic Video Analysis of Historical Films",
booktitle="New Trends in Image Analysis and Processing -- ICIAP 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="137--147",
abstract="In automatic video content analysis and film preservation, Shot Boundary Detection (SBD) is a fundamental pre-processing step. While previous research focuses on detecting Abrupt Transitions (AT) as well as Gradual Transitions (GT) in different video genres such as sports movies or news clips only few studies investigate in the detection of shot transitions in historical footage. The main aim of this paper is to create an SBD mechanism inspired by state-of-the-art algorithms which is applied and evaluated on a self-generated historical dataset as well as a publicly available dataset called Clipshots. Therefore, a three-stage pipeline is introduced consisting of a Candidate Frame Range Selection based on the network DeepSBD, Extraction of Convolutional Neural Network (CNN) Features and Similarity Calculation. A combination of pre-trained backbone CNNs such as ResNet, VGG19 and SqueezeNet with different similarity metrics like Cosine Similarity and Euclidean Distance are used and evaluated. The outcome of this paper displays that the proposed algorithm reaches promising results on detecting ATs in historical videos without the need of complex optimization and re-training processes. Furthermore, it points out the main challenges concerning historical footage such as damaged film reels, scratches or splices. The results of this paper contribute a significant base for future research on automatic video analysis of historical videos.",
isbn="978-3-030-30754-7"
}

@article{CLIP,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               others},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {CoRR},
  volume    = {abs/2103.00020},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  eprinttype = {arXiv},
  eprint    = {2103.00020},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@article{2019Moviescope,
title={Moviescope: Large-scale Analysis of Movies using Multiple Modalities},
author={Paola Cascante-Bonilla and Kalpathy Sitaraman and Mengjia Luo and Vicente Ordonez},
journal={ArXiv},
year={2019},
volume={abs/1908.03180}
}

@INPROCEEDINGS{IndoorScenes,
  author={Quattoni, Ariadna and Torralba, Antonio},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Recognizing indoor scenes}, 
  year={2009},
  volume={},
  number={},
  pages={413-420},
  doi={10.1109/CVPR.2009.5206537}}

@article{xiao_sun_2016,
	title = {{SUN} {Database}: {Exploring} a {Large} {Collection} of {Scene} {Categories}},
	volume = {119},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-014-0748-y},
	doi = {10.1007/s11263-014-0748-y},
	abstract = {Progress in scene understanding requires reasoning about the rich and diverse visual environments that make up our daily experience. To this end, we propose the Scene Understanding database, a nearly exhaustive collection of scenes categorized at the same level of specificity as human discourse. The database contains 908 distinct scene categories and 131,072 images. Given this data with both scene and object labels available, we perform in-depth analysis of co-occurrence statistics and the contextual relationship. To better understand this large scale taxonomy of scene categories, we perform two human experiments: we quantify human scene recognition accuracy, and we measure how typical each image is of its assigned scene category. Next, we perform computational experiments: scene recognition with global image features, indoor versus outdoor classification, and “scene detection,” in which we relax the assumption that one image depicts only one scene category. Finally, we relate human experiments to machine performance and explore the relationship between human and machine recognition errors and the relationship between image “typicality” and machine recognition accuracy.},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Xiao, Jianxiong and Ehinger, Krista A. and Hays, James and Torralba, Antonio and Oliva, Aude},
	month = aug,
	year = {2016},
	pages = {3--22},
}

@article{kinetics400,
  author    = {Will Kay and
               Jo{\~{a}}o Carreira and
               Karen Simonyan and
               Brian Zhang and
               Chloe Hillier and
               Sudheendra Vijayanarasimhan and
               Fabio Viola and
               Tim Green and
               Trevor Back and
               Paul Natsev and
               Mustafa Suleyman and
               Andrew Zisserman},
  title     = {The Kinetics Human Action Video Dataset},
  journal   = {CoRR},
  volume    = {abs/1705.06950},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.06950},
  eprinttype = {arXiv},
  eprint    = {1705.06950},
  timestamp = {Thu, 14 Oct 2021 09:15:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KayCSZHVVGBNSZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{caba2015activitynet,
  title={ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding},
  author={Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem and Juan Carlos Niebles},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={961--970},
  year={2015}
}

@article{Kinetics700,
  author    = {Jo{\~{a}}o Carreira and
               Eric Noland and
               Chloe Hillier and
               Andrew Zisserman},
  title     = {A Short Note on the Kinetics-700 Human Action Dataset},
  journal   = {CoRR},
  volume    = {abs/1907.06987},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.06987},
  eprinttype = {arXiv},
  eprint    = {1907.06987},
  timestamp = {Tue, 23 Jul 2019 10:54:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-06987.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gu2018ava,
  title={Ava: A video dataset of spatio-temporally localized atomic visual actions},
  author={Gu, Chunhui and Sun, Chen and Ross, David A and Vondrick, Carl and Pantofaru, Caroline and others},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6047--6056},
  year={2018}
}
  
@article{Something-SomethingV2,
  author    = {Raghav Goyal and
               Samira Ebrahimi Kahou and
               Vincent Michalski and
               Joanna Materzynska and
               Susanne Westphal and others},
  title     = {The "something something" video database for learning and evaluating
               visual common sense},
  journal   = {CoRR},
  volume    = {abs/1706.04261},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.04261},
  eprinttype = {arXiv},
  eprint    = {1706.04261},
  timestamp = {Mon, 13 Aug 2018 16:48:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalKMMWKHFYMH17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{SOA,
author="Ray, Jamie
and Wang, Heng
and Tran, Du
and Wang, Yufei
and Feiszli, Matt
and Torresani, Lorenzo
and Paluri, Manohar",
title="Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="660--676",
abstract="This paper introduces a large-scale, multi-label and multi-task video dataset named Scenes-Objects-Actions (SOA). Most prior video datasets are based on a predefined taxonomy, which is used to define the keyword queries issued to search engines. The videos retrieved by the search engines are then verified for correctness by human annotators. Datasets collected in this manner tend to generate high classification accuracy as search engines typically rank ``easy'' videos first. The SOA dataset adopts a different approach. We rely on uniform sampling to get a better representation of videos on the Web. Trained annotators are asked to provide free-form text labels describing each video in three different aspects: scene, object and action. These raw labels are then merged, split and renamed to generate a taxonomy for SOA. All the annotations are verified again based on the taxonomy. The final dataset includes 562K videos with 3.64M annotations spanning 49 categories for scenes, 356 for objects, 148 for actions, and naturally captures the long tail distribution of visual concepts in the real world. We show that datasets collected in this way are quite challenging by evaluating existing popular video models on SOA. We provide in-depth analysis about the performance of different models on SOA, and highlight potential new directions in video classification. We compare SOA with existing datasets and discuss various factors that impact the performance of transfer learning. A key-feature of SOA is that it enables the empirical study of correlation among scene, object and action recognition in video. We present results of this study and further analyze the potential of using the information learned from one task to improve the others. We also demonstrate different ways of scaling up SOA to learn better features. We believe that the challenges presented by SOA offer the opportunity for further advancement in video analysis as we progress from single-label classification towards a more comprehensive understanding of video data.",
isbn="978-3-030-01264-9"
}

@inproceedings{diba_large_2020,
	address = {Cham},
	title = {Large {Scale} {Holistic} {Video} {Understanding}},
	isbn = {978-3-030-58558-7},
	abstract = {Video recognition has been advanced in recent years by benchmarks with rich annotations. However, research is still mainly limited to human action or sports recognition - focusing on a highly specific video understanding task and thus leaving a significant gap towards describing the overall content of a video. We fill this gap by presenting a large-scale “Holistic Video Understanding Dataset” (HVU). HVU is organized hierarchically in a semantic taxonomy that focuses on multi-label and multi-task video understanding as a comprehensive problem that encompasses the recognition of multiple semantic aspects in the dynamic scene. HVU contains approx. 572k videos in total with 9 million annotations for training, validation and test set spanning over 3142 labels. HVU encompasses semantic aspects defined on categories of scenes, objects, actions, events, attributes and concepts which naturally captures the real-world scenarios.},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Paluri, Manohar and Gall, Jürgen and Stiefelhagen, Rainer and Van Gool, Luc},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {593--610},
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@InProceedings{marszalek09,
    author = "Marcin Marsza{\l}ek and Ivan Laptev and Cordelia Schmid",
    title = "Actions in Context",
    booktitle = "IEEE Conference on Computer Vision \& Pattern Recognition",
    year = "2009"
}

@inproceedings{moviegraphs,
  title={MovieGraphs: Towards Understanding Human-Centric Situations from Videos},
  author={Paul Vicol and Makarand Tapaswi and Lluis Castrejon and Sanja Fidler},
  booktitle={{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}},
  year={2018}
}

@inproceedings{huang2020movienet,
	title={MovieNet: A Holistic Dataset for Movie Understanding},
	author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
	booktitle = {The European Conference on Computer Vision (ECCV)}, 
	year={2020}
}

@inproceedings{lvu2021,
  Author    = {Chao-Yuan Wu and Philipp Kr\"{a}henb\"{u}hl},
  Title     = {{Towards Long-Form Video Understanding}},
  Booktitle = {{CVPR}},
  Year      = {2021}}


@inproceedings{transformers,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@INPROCEEDINGS{BayesianFeiFeiLi,
  author={Fei-Fei, L. and Perona, P.},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
  title={A Bayesian hierarchical model for learning natural scene categories}, 
  year={2005},
  volume={2},
  number={},
  pages={524-531 vol. 2},
  doi={10.1109/CVPR.2005.16}}

@InProceedings{sariyildiz2020icmlm,
author = {Sariyildiz, Mert Bulent and Perez, Julien and Larlus, Diane},
title = {Learning Visual Representations with Caption Annotations},
booktitle = {European Conference on Computer Vision (ECCV)},
year = {2020}
}

@inproceedings{desai2021virtex,
    title={{VirTex: Learning Visual Representations from Textual Annotations}},
    author={Karan Desai and Justin Johnson},
    booktitle={CVPR},
    year={2021}
}

@article{Shen2021HowMC,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Sheng Shen and Liunian Harold Li and Hao Tan and Mohit Bansal and Anna Rohrbach and Kai-Wei Chang and Zhewei Yao and Kurt Keutzer},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.06383}
}

@inproceedings{Gu2021OpenvocabularyOD,
  title={Open-vocabulary Object Detection via Vision and Language Knowledge Distillation},
  author={Xiuye Gu and Tsung-Yi Lin and Weicheng Kuo and Yin Cui},
  year={2021}
}

@article{Li2022LanguagedrivenSS,
  title={Language-driven Semantic Segmentation},
  author={Boyi Li and Kilian Q. Weinberger and Serge J. Belongie and Vladlen Koltun and Ren{\'e} Ranftl},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.03546}
}

 @InProceedings{10.1007/978-3-540-88693-8_12,
author="Cour, Timothee
and Jordan, Chris
and Miltsakaki, Eleni
and Taskar, Ben",
editor="Forsyth, David
and Torr, Philip
and Zisserman, Andrew",
title="Movie/Script: Alignment and Parsing of Video and Text Transcription",
booktitle="Computer Vision -- ECCV 2008",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="158--171",
abstract="Movies and TV are a rich source of diverse and complex video of people, objects, actions and locales ``in the wild''. Harvesting automatically labeled sequences of actions from video would enable creation of large-scale and highly-varied datasets. To enable such collection, we focus on the task of recovering scene structure in movies and TV series for object tracking and action retrieval. We present a weakly supervised algorithm that uses the screenplay and closed captions to parse a movie into a hierarchy of shots and scenes. Scene boundaries in the movie are aligned with screenplay scene labels and shots are reordered into a sequence of long continuous tracks or threads which allow for more accurate tracking of people, actions and objects. Scene segmentation, alignment, and shot threading are formulated as inference in a unified generative model and a novel hierarchical dynamic programming algorithm that can handle alignment and jump-limited reorderings in linear time is presented. We present quantitative and qualitative results on movie alignment and parsing, and use the recovered structure to improve character naming and retrieval of common actions in several episodes of popular TV series.",
isbn="978-3-540-88693-8"
}

@inproceedings{Everingham2006HelloMN,
  title={Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video},
  author={Mark Everingham and Josef Sivic and Andrew Zisserman},
  booktitle={BMVC},
  year={2006}
}

@article{Rohrbach2015ADF,
  title={A dataset for Movie Description},
  author={Anna Rohrbach and Marcus Rohrbach and Niket Tandon and Bernt Schiele},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={3202-3212}
}

@INPROCEEDINGS{Laptevactionscvpr,
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Learning realistic human actions from movies}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/CVPR.2008.4587756}}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@article{brendanfrey,
author = {Brendan J. Frey  and Delbert Dueck},
title = {Clustering by Passing Messages Between Data Points},
journal = {Science},
volume = {315},
number = {5814},
pages = {972-976},
year = {2007},
doi = {10.1126/science.1136800},
URL = {https://www.science.org/doi/abs/10.1126/science.1136800},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1136800},
abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such “exemplars” can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.}}


@article{tSNE,
  author  = {Laurens van der Maaten and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579-2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@inproceedings{BarsoumICMI2016,
    title={Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution},
    author={Barsoum, Emad and Zhang, Cha and Canton Ferrer, Cristian and Zhang, Zhengyou},
    booktitle={ACM International Conference on Multimodal Interaction (ICMI)},
    year={2016}
}

@article{Deng2009ImageNetAL,
  title={ImageNet: A large-scale hierarchical image database},
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  pages={248-255}
}

@InProceedings{parkhi12a,
  author       = "Parkhi, O. M. and Vedaldi, A. and Zisserman, A. and Jawahar, C.~V.",
  title        = "Cats and Dogs",
  booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
  year         = "2012",
}
@inproceedings{bossard14,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}

@inproceedings{KrauseStarkDengFei-Fei_3DRR2013,
  title = {3D Object Representations for Fine-Grained Categorization},
  booktitle = {4th International IEEE Workshop on  3D Representation and Recognition (3dRR-13)},
  year = {2013},
  address = {Sydney, Australia},
  author = {Jonathan Krause and Michael Stark and Jia Deng and Li Fei-Fei}
}

@article{Deng2009ImageNetAL,
  title={ImageNet: A large-scale hierarchical image database},
  author={Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
  journal={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year={2009},
  pages={248-255}
}

@article{FeiFei2004LearningGV,
  title={Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},
  author={Li Fei-Fei and Rob Fergus and Pietro Perona},
  journal={Computer Vision and Pattern Recognition Workshop},
  year={2004},
}

@article{ucf101,
  author    = {Khurram Soomro and
               Amir Roshan Zamir and
               Mubarak Shah},
  title     = {{UCF101:} {A} Dataset of 101 Human Actions Classes From Videos in
               The Wild},
  journal   = {CoRR},
  volume    = {abs/1212.0402},
  year      = {2012},
  url       = {http://arxiv.org/abs/1212.0402},
  eprinttype = {arXiv},
  eprint    = {1212.0402},
  timestamp = {Mon, 13 Aug 2018 16:47:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1212-0402.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{rao2020unified,
title={A Unified Framework for Shot Type Classification Based on Subject Centric Lens},
author={Rao, Anyi and Wang, Jiaze and Xu, Linning and Jiang, Xuekun and Huang, Qingqiu and Zhou, Bolei and Lin, Dahua},
booktitle = {The European Conference on Computer Vision (ECCV)}, 
year={2020}
}

@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
  }

@article{lstm,
  added-at = {2016-11-15T08:49:43.000+0100},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  biburl = {https://www.bibsonomy.org/bibtex/2a4a80026d24955b267cae636aa8abe4a/dallmann},
  interhash = {0692b471c4b9ae65d00affebc09fb467},
  intrahash = {a4a80026d24955b267cae636aa8abe4a},
  journal = {Neural computation},
  keywords = {lstm rnn},
  number = 8,
  pages = {1735--1780},
  publisher = {MIT Press},
  timestamp = {2016-11-15T08:49:43.000+0100},
  title = {Long short-term memory},
  volume = 9,
  year = 1997
}

@incollection{Pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and others},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{i3d,
  author = {Carreira, J. and Zisserman, Andrew},
  year = {2017},
  month = {07},
  pages = {4724-4733},
  title = {Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset},
  doi = {10.1109/CVPR.2017.502}
}
@inproceedings{r2plus1d,
  title={A closer look at spatiotemporal convolutions for action recognition},
  author={Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={6450--6459},
  year={2018}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={6202--6211},
  year={2019}
}

@article{Bertasius2021IsSA,
  title={Is Space-Time Attention All You Need for Video Understanding?},
  author={Gedas Bertasius and Heng Wang and Lorenzo Torresani},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.05095}
}

@article{liu2021video,
  title={Video Swin Transformer},
  author={Liu, Ze and Ning, Jia and Cao, Yue and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Hu, Han},
  journal={arXiv preprint arXiv:2106.13230},
  year={2021}
}

@article{liu2021Swin,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  journal={arXiv preprint arXiv:2103.14030},
  year={2021}
}

@ARTICLE{DuTran,
author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
title = {Learning Spatiotemporal Features with 3D Convolutional Networks},
keywords = {Computer Science - Computer Vision and Pattern Recognition},
year = 2014,
month = dec,
eid = {arXiv:1412.0767}
}

@article{He2015,
	author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	title = {Deep Residual Learning for Image Recognition},
	journal = {arXiv preprint arXiv:1512.03385},
	year = {2015}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}  

@article{dukes2021,
  title={The rise of affectivism},
  author={Dukes, Daniel and Abrams, Kathryn and Adolphs, Ralph and Ahmed, Mohammed E and Beatty, Andrew and Berridge, Kent C and others},
  journal={Nature human behaviour},
  volume={5},
  number={7},
  pages={816--820},
  year={2021},
  publisher={Nature Publishing Group}
}

@ARTICLE{AICA,
  author={Zhao, Sicheng and Yao, Xingxu and Yang, Jufeng and Jia, Guoli and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Affective Image Content Analysis: Two Decades Review and New Perspectives}, 
  year={2022},
  volume={44},
  number={10},
  pages={6729-6751},
  doi={10.1109/TPAMI.2021.3094362}}

@ARTICLE {speechemo,
author = {S. Latif and R. Rana and S. Khalifa and R. Jurdak and J. Qadir and B. W. Schuller},
journal = {IEEE Transactions on Affective Computing},
title = {Survey of Deep Representation Learning for Speech Emotion Recognition},
year = {5555},
volume = {},
number = {01},
issn = {1949-3045},
pages = {1-1},
abstract = {Traditionally, speech emotion recognition (SER) research has relied on manually handcrafted acoustic features using feature engineering. However, the design of handcrafted features for complex SER tasks requires significant manual eort, which impedes generalisability and slows the pace of innovation. This has motivated the adoption of representation learning techniques that can automatically learn an intermediate representation of the input signal without any manual feature engineering. Representation learning has led to improved SER performance and enabled rapid innovation. Its effectiveness has further increased with advances in deep learning (DL), which has facilitated \textit{deep representation learning} where hierarchical representations are automatically learned in a data-driven manner. This paper presents the first comprehensive survey on the important topic of deep representation learning for SER. We highlight various techniques, related challenges and identify important future areas of research. Our survey bridges the gap in the literature since existing surveys either focus on SER with hand-engineered features or representation learning in the general setting without focusing on SER.},
keywords = {principal component analysis;task analysis;speech recognition;emotion recognition;deep learning;australia;neurons},
doi = {10.1109/TAFFC.2021.3114365},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep}
}

@article{Poria2019EmotionRI,
  title={Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances},
  author={Soujanya Poria and Navonil Majumder and Rada Mihalcea and Eduard H. Hovy},
  journal={IEEE Access},
  year={2019},
  volume={7},
  pages={100943-100953},
  url={https://api.semanticscholar.org/CorpusID:147703962}
}

@INPROCEEDINGS{depressiondetection,
  author={Zucco, Chiara and Calabrese, Barbara and Cannataro, Mario},
  booktitle={2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
  title={Sentiment analysis and affective computing for depression monitoring}, 
  year={2017},
  volume={},
  number={},
  pages={1988-1995},
  doi={10.1109/BIBM.2017.8217966}}

@ARTICLE{autismguha,
  author={Guha, Tanaya and Yang, Zhaojun and Grossman, Ruth B. and Narayanan, Shrikanth S.},
  journal={IEEE Transactions on Affective Computing}, 
  title={A Computational Study of Expressive Facial Dynamics in Children with Autism}, 
  year={2018},
  volume={9},
  number={1},
  pages={14-20},
  doi={10.1109/TAFFC.2016.2578316}}

@ARTICLE{savchecnkoengagement,
  author={Savchenko, Andrey V. and Savchenko, Lyudmila V. and Makarov, Ilya},
  journal={IEEE Transactions on Affective Computing}, 
  title={Classifying emotions and engagement in online learning based on a single facial expression recognition neural network}, 
  year={2022},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TAFFC.2022.3188390}}

@inproceedings{DFEW,
author = {Jiang, Xingxun and Zong, Yuan and others},
title = {DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413620},
doi = {10.1145/3394171.3413620},
abstract = {Recently, facial expression recognition (FER) in the wild has gained a lot of researchers' attention because it is a valuable topic to enable the FER techniques to move from the laboratory to the real applications. In this paper, we focus on this challenging but interesting topic and make contributions from three aspects. First, we present a new large-scale 'in-the-wild' dynamic facial expression database, DFEW (Dynamic Facial Expression in the Wild), consisting of over 16,000 video clips from thousands of movies. These video clips contain various challenging interferences in practical scenarios such as extreme illumination, occlusions, and capricious pose changes. Second, we propose a novel method called Expression-Clustered Spatiotemporal Feature Learning (EC-STFL) framework to deal with dynamic FER in the wild. Third, we conduct extensive benchmark experiments on DFEW using a lot of spatiotemporal deep feature learning methods as well as our proposed EC-STFL. Experimental results show that DFEW is a well-designed and challenging database, and the proposed EC-STFL can promisingly improve the performance of existing spatiotemporal deep neural networks in coping with the problem of dynamic FER in the wild. Our DFEW database is publicly available and can be freely downloaded from https://dfew-dataset.github.io/.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2881–2889},
numpages = {9},
keywords = {in-the-wild facial expression recognition, dynamic facial expression, facial expression database, deep learning},
location = {Seattle, WA, USA},
series = {MM '20}
}

@article{Mollahosseini2019AffectNetAD,
  title={AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild},
  author={Ali Mollahosseini and Behzad Hasani and Mohammad H. Mahoor},
  journal={IEEE Transactions on Affective Computing},
  year={2019},
  volume={10},
  pages={18-31}
}
